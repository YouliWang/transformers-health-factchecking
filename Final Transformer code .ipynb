{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:16:45.065779Z",
     "iopub.status.busy": "2025-07-15T05:16:45.065291Z",
     "iopub.status.idle": "2025-07-15T05:16:47.626667Z",
     "shell.execute_reply": "2025-07-15T05:16:47.625855Z",
     "shell.execute_reply.started": "2025-07-15T05:16:45.065751Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/glove-cache/glove-cache/glove_monant_5000vocab_300d.npy\n",
      "/kaggle/input/glove-cache/glove-cache/glove_med_5000vocab_300d.npy\n",
      "/kaggle/input/glove-cache/glove-cache/desktop.ini\n",
      "/kaggle/input/glove-cache/glove-cache/glove_pubhealth_5000vocab_300d.npy\n",
      "/kaggle/input/glove-6b-300d/glove.6B.300d.txt\n",
      "/kaggle/input/final-data-for-training/Final data for training/Monant_claims.csv\n",
      "/kaggle/input/final-data-for-training/Final data for training/desktop.ini\n",
      "/kaggle/input/final-data-for-training/Final data for training/PUBHEALTH/test.tsv\n",
      "/kaggle/input/final-data-for-training/Final data for training/PUBHEALTH/dev.tsv\n",
      "/kaggle/input/final-data-for-training/Final data for training/PUBHEALTH/train.tsv\n",
      "/kaggle/input/final-data-for-training/Final data for training/PUBHEALTH/desktop.ini\n",
      "/kaggle/input/final-data-for-training/Final data for training/Med-MMHL/dev.csv\n",
      "/kaggle/input/final-data-for-training/Final data for training/Med-MMHL/desktop.ini\n",
      "/kaggle/input/final-data-for-training/Final data for training/Med-MMHL/train.csv\n",
      "/kaggle/input/final-data-for-training/Final data for training/Med-MMHL/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:16:47.628652Z",
     "iopub.status.busy": "2025-07-15T05:16:47.628101Z",
     "iopub.status.idle": "2025-07-15T05:16:52.819295Z",
     "shell.execute_reply": "2025-07-15T05:16:52.818579Z",
     "shell.execute_reply.started": "2025-07-15T05:16:47.628632Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT.ipynb\\n\\nAutomatically generated by Colab.\\n\\nOriginal file is located at\\n    https://colab.research.google.com/drive/1Q0YHA6RUjwOClr1AKzK3nUcPlVusNKGa\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required packages for Excel output\n",
    "!pip install -q openpyxl\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"BERT.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1Q0YHA6RUjwOClr1AKzK3nUcPlVusNKGa\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:16:52.820481Z",
     "iopub.status.busy": "2025-07-15T05:16:52.820237Z",
     "iopub.status.idle": "2025-07-15T05:16:59.123165Z",
     "shell.execute_reply": "2025-07-15T05:16:59.122503Z",
     "shell.execute_reply.started": "2025-07-15T05:16:52.820459Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is Available\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is NOT available.\")\n",
    "    print(\"Please go to Runtime > Change runtime type > Select GPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:16:59.124468Z",
     "iopub.status.busy": "2025-07-15T05:16:59.124022Z",
     "iopub.status.idle": "2025-07-15T05:19:00.619000Z",
     "shell.execute_reply": "2025-07-15T05:19:00.618238Z",
     "shell.execute_reply.started": "2025-07-15T05:16:59.124415Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752556722.241814      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752556722.342387      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets accelerate evaluate optuna\n",
    "\n",
    "# ===== Imports ===== #\n",
    "# Import all the libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import openpyxl\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from evaluate import load as load_metric\n",
    "from time import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:00.621406Z",
     "iopub.status.busy": "2025-07-15T05:19:00.620928Z",
     "iopub.status.idle": "2025-07-15T05:19:00.633783Z",
     "shell.execute_reply": "2025-07-15T05:19:00.633147Z",
     "shell.execute_reply.started": "2025-07-15T05:19:00.621383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Seed Everything (for full reproducibility) ===== #\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False  # This makes sure cuDNN doesn’t auto-tune\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:00.634789Z",
     "iopub.status.busy": "2025-07-15T05:19:00.634553Z",
     "iopub.status.idle": "2025-07-15T05:19:00.655705Z",
     "shell.execute_reply": "2025-07-15T05:19:00.655060Z",
     "shell.execute_reply.started": "2025-07-15T05:19:00.634768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove old outputs directory (optional but recommended for clean experiments)\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"/kaggle/working/MSc_Claim_Experiment/outputs\", ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:00.656910Z",
     "iopub.status.busy": "2025-07-15T05:19:00.656479Z",
     "iopub.status.idle": "2025-07-15T05:19:00.673484Z",
     "shell.execute_reply": "2025-07-15T05:19:00.672872Z",
     "shell.execute_reply.started": "2025-07-15T05:19:00.656867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===== Configurations ===== #\n",
    "\n",
    "# model_name = \"bert-base-uncased\"  # Use the standard model √\n",
    "# model_name = \"distilbert-base-uncased\"   √\n",
    "# model_name = \"roberta-base\"  √\n",
    "# model_name = \"albert-base-v2\"  √\n",
    "model_name = \"xlnet-base-cased\" \n",
    "# model_name = \"google/electra-base-discriminator\" \n",
    "\n",
    "# lr = 2e-5  # Learning rate (changeable)\n",
    "\n",
    "\n",
    "timestamp = datetime.now().isoformat()\n",
    "\n",
    "# Generate a unique run ID with model name and timestamp\n",
    "# run_id = f\"{model_name.replace('/', '_')}_lr{str(lr).replace('.', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "\n",
    "run_id = f\"{model_name.replace('/', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# ===== Environment-Aware Path Setup ===== #\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Running in Google Colab\n",
    "    project_root = Path(\"/content/drive/MyDrive/MSc_Claim_Experiment\")\n",
    "elif Path(\"/kaggle/working\").exists():\n",
    "    # Running in Kaggle\n",
    "    project_root = Path(\"/kaggle/working/MSc_Claim_Experiment\")\n",
    "else:\n",
    "    # Fallback for local development\n",
    "    project_root = Path(\"./MSc_Claim_Experiment\")\n",
    "\n",
    "# Define the output directory for the current run\n",
    "# output_dir = project_root / \"outputs\" / \"results\" / run_id\n",
    "output_dir = project_root / \"outputs\" / \"results\" / model_name.replace(\"/\", \"_\") / run_id\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create directory if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:00.674494Z",
     "iopub.status.busy": "2025-07-15T05:19:00.674260Z",
     "iopub.status.idle": "2025-07-15T05:19:01.033616Z",
     "shell.execute_reply": "2025-07-15T05:19:01.032782Z",
     "shell.execute_reply.started": "2025-07-15T05:19:00.674468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Load and Preprocess Data ===== #\n",
    "\n",
    "# Select dataset name to control which source is loaded\n",
    "# dataset_name = \"monant\" \n",
    "# dataset_name = \"pubhealth\" \n",
    "dataset_name = \"med\" \n",
    "\n",
    "if dataset_name == \"monant\":\n",
    "    # Load Monant dataset and standardize labels\n",
    "    # df = pd.read_csv(\"/kaggle/input/final-data-for-training/Monant_claims.csv\", encoding=\"ISO-8859-1\", engine=\"python\")\n",
    "    df = pd.read_csv(\"/kaggle/input/final-data-for-training/Final data for training/Monant_claims.csv\", encoding=\"ISO-8859-1\", engine=\"python\")\n",
    "\n",
    "    df['rating'] = df['rating'].str.lower().str.strip()  # Make ratings lowercase and remove spaces\n",
    "\n",
    "     # Map 4-class labels to binary classification: 1 for true/mostly-true, 0 for false/mostly-false\n",
    "    mapping = {\n",
    "        'true': 1,\n",
    "        'mostly-true': 1,\n",
    "        'false': 0,\n",
    "        'mostly-false': 0\n",
    "}  \n",
    "    \n",
    "    # Apply label mapping\n",
    "    df['label_binary'] = df['rating'].map(mapping)\n",
    "    df = df[df['label_binary'].isin([0, 1])] # Keep only rows that are 0 or 1 (drop unknowns)\n",
    "\n",
    "    # Split into train, validation, test\n",
    "    train_val, test = train_test_split(df, test_size=0.15, stratify=df['label_binary'], random_state=42)\n",
    "    train, val      = train_test_split(train_val, test_size=0.1765, stratify=train_val['label_binary'], random_state=42)\n",
    "\n",
    " # Ensure 'statement' is the standard text column for all datasets\n",
    "    for df_ in [train, val, test]:\n",
    "        df_['statement'] = df_['statement']  # Redundant if column already exists; safe fallback\n",
    "\n",
    "elif dataset_name == \"pubhealth\":\n",
    "    # Load PubHealth dataset (already split into train/dev/test)\n",
    "    train = pd.read_csv(\"/kaggle/input/final-data-for-training/Final data for training/PUBHEALTH/train.tsv\", sep=\"\\t\")\n",
    "    val   = pd.read_csv(\"/kaggle/input/final-data-for-training/Final data for training/PUBHEALTH/dev.tsv\", sep=\"\\t\")\n",
    "    test  = pd.read_csv(\"/kaggle/input/final-data-for-training/Final data for training/PUBHEALTH/test.tsv\", sep=\"\\t\")\n",
    "   \n",
    "    # Map labels to binary classification and align column names\n",
    "    mapping = {'true': 1, 'mostly true': 1, 'false': 0, 'mostly false': 0}\n",
    "    \n",
    "    for df_ in [train, val, test]:\n",
    "        # Standardize and map labels\n",
    "        df_['label'] = df_['label'].astype(str).str.lower().str.strip()\n",
    "        df_['label_binary'] = df_['label'].map(mapping)\n",
    "\n",
    "        # Align main text field to common column name 'statement'\n",
    "        df_['statement'] = df_['claim'] \n",
    "\n",
    "    # Keep only rows with valid binary labels\n",
    "    train, val, test = [df[df['label_binary'].isin([0, 1])] for df in [train, val, test]]\n",
    "\n",
    "elif dataset_name == \"med\":\n",
    "    train = pd.read_csv(\"/kaggle/input/final-data-for-training/Final data for training/Med-MMHL/train.csv\")\n",
    "    val   = pd.read_csv(\"/kaggle/input/final-data-for-training/Final data for training/Med-MMHL/dev.csv\")\n",
    "    test  = pd.read_csv(\"/kaggle/input/final-data-for-training/Final data for training/Med-MMHL/test.csv\")\n",
    "   \n",
    "      # Align field names for consistency across datasets\n",
    "    for df_ in [train, val, test]:\n",
    "        df_['label_binary'] = df_['det_fake_label']  # Already 0 or 1\n",
    "        df_['statement'] = df_['content']            # Standardize text column\n",
    "        df_.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')  # Drop extra index if exists\n",
    "\n",
    "    # Optional: Ensure only binary labels 0 or 1 remain (in case of data corruption)\n",
    "    train, val, test = [df[df['label_binary'].isin([0, 1])] for df in [train, val, test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:01.034783Z",
     "iopub.status.busy": "2025-07-15T05:19:01.034518Z",
     "iopub.status.idle": "2025-07-15T05:19:01.258121Z",
     "shell.execute_reply": "2025-07-15T05:19:01.257424Z",
     "shell.execute_reply.started": "2025-07-15T05:19:01.034766Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: label_binary\n",
      "0    0.70142\n",
      "1    0.29858\n",
      "Name: proportion, dtype: float64\n",
      "Val: label_binary\n",
      "0    0.701356\n",
      "1    0.298644\n",
      "Name: proportion, dtype: float64\n",
      "Test: label_binary\n",
      "0    0.701399\n",
      "1    0.298601\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ===== Compute Class Weights =====\n",
    "# Handle class imbalance by adjusting loss contribution of each class. Give rare class more weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train['label_binary']),\n",
    "    y=train['label_binary']\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Save class weights to JSON file for reproducibility\n",
    "with open(output_dir / \"class_weights.json\", \"w\") as f:\n",
    "    json.dump(class_weights.cpu().numpy().tolist(), f, indent=2)\n",
    "\n",
    "# Show class distribution\n",
    "print(\"Train:\", train['label_binary'].value_counts(normalize=True))\n",
    "print(\"Val:\", val['label_binary'].value_counts(normalize=True))\n",
    "print(\"Test:\", test['label_binary'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:01.259123Z",
     "iopub.status.busy": "2025-07-15T05:19:01.258863Z",
     "iopub.status.idle": "2025-07-15T05:19:01.364953Z",
     "shell.execute_reply": "2025-07-15T05:19:01.364111Z",
     "shell.execute_reply.started": "2025-07-15T05:19:01.259098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Column Cleanup Before HuggingFace Dataset Conversion ===== #\n",
    "columns_to_keep = ['statement', 'label_binary']\n",
    "cleaned_train = train[columns_to_keep].copy()\n",
    "cleaned_val   = val[columns_to_keep].copy()\n",
    "cleaned_test  = test[columns_to_keep].copy()\n",
    "\n",
    "# Combine for tokenization\n",
    "dataset = Dataset.from_pandas(\n",
    "    pd.concat([cleaned_train, cleaned_val, cleaned_test]).reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:01.366842Z",
     "iopub.status.busy": "2025-07-15T05:19:01.365853Z",
     "iopub.status.idle": "2025-07-15T05:19:01.375905Z",
     "shell.execute_reply": "2025-07-15T05:19:01.375101Z",
     "shell.execute_reply.started": "2025-07-15T05:19:01.366764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===== Ensure Labels Are Integers (Fix for loss mismatch) ===== #\n",
    "# Prevents loss function errors by ensuring labels are int (not string or float)\n",
    "train = train.copy()\n",
    "val = val.copy()\n",
    "test = test.copy()\n",
    "\n",
    "train[\"label_binary\"] = train[\"label_binary\"].astype(int)\n",
    "val[\"label_binary\"] = val[\"label_binary\"].astype(int)\n",
    "test[\"label_binary\"] = test[\"label_binary\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:01.377355Z",
     "iopub.status.busy": "2025-07-15T05:19:01.376818Z",
     "iopub.status.idle": "2025-07-15T05:19:02.556523Z",
     "shell.execute_reply": "2025-07-15T05:19:02.555702Z",
     "shell.execute_reply.started": "2025-07-15T05:19:01.377331Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8c7410b3b242a6ae71d7d381d22f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57de6e82684d4c2fa808c1f437a939d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c976e9e2963b417eb23579da5b8938bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ===== Tokenizer ===== #\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load tokenizer\n",
    "\n",
    "def tokenize(batch):\n",
    "    # Try statement first, fall back to content\n",
    "    if 'statement' in batch:\n",
    "        texts = batch['statement']\n",
    "    elif 'content' in batch:\n",
    "        texts = batch['content']\n",
    "    else:\n",
    "        raise ValueError(\"No valid text field found in batch.\")\n",
    "    \n",
    "    return tokenizer(texts, padding=True, truncation=True)  # Turn text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:02.557747Z",
     "iopub.status.busy": "2025-07-15T05:19:02.557409Z",
     "iopub.status.idle": "2025-07-15T05:19:02.561442Z",
     "shell.execute_reply": "2025-07-15T05:19:02.560868Z",
     "shell.execute_reply.started": "2025-07-15T05:19:02.557714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Columns to Remove During Cleanup ===== #\n",
    "columns_to_remove = [\n",
    "    'id', 'name', 'description', 'category', 'rating',\n",
    "    'claim_id', 'source_url', 'queries', 'created_at', 'updated_at',\n",
    "    'Unnamed: 0', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13',\n",
    "    'statement', 'claim','content', 'date_published', 'explanation',\n",
    "    'main_text', 'fact_checkers', 'sources', 'subjects'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:02.565416Z",
     "iopub.status.busy": "2025-07-15T05:19:02.565216Z",
     "iopub.status.idle": "2025-07-15T05:19:02.584991Z",
     "shell.execute_reply": "2025-07-15T05:19:02.584402Z",
     "shell.execute_reply.started": "2025-07-15T05:19:02.565402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary columns dynamically\n",
    "def safe_remove_columns(dataset, columns):\n",
    "    existing = [col for col in columns if col in dataset.column_names]\n",
    "    return dataset.remove_columns(existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:02.585966Z",
     "iopub.status.busy": "2025-07-15T05:19:02.585717Z",
     "iopub.status.idle": "2025-07-15T05:19:02.603726Z",
     "shell.execute_reply": "2025-07-15T05:19:02.603163Z",
     "shell.execute_reply.started": "2025-07-15T05:19:02.585945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Unified Tokenize + Clean Function ===== #\n",
    "def tokenize_and_clean(df):\n",
    "    \"\"\"\n",
    "    Tokenize and clean a DataFrame for Transformer input.\n",
    "    - Ensures labels are renamed to 'labels' (required by Trainer).\n",
    "    - Removes unnecessary columns, including misleading 'label' if it's text.\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== Step 0: Convert to HuggingFace Dataset ===== #\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False)  \n",
    "\n",
    "    # ===== Step 1: Identify and rename the correct label column ===== #\n",
    "    if \"label_binary\" in ds.column_names:\n",
    "        ds = ds.rename_column(\"label_binary\", \"labels\")\n",
    "    elif \"det_fake_label\" in ds.column_names:\n",
    "        ds = ds.rename_column(\"det_fake_label\", \"labels\")\n",
    "    elif \"label\" in ds.column_names and isinstance(ds[\"label\"][0], int):\n",
    "        ds = ds.rename_column(\"label\", \"labels\")\n",
    "    else:\n",
    "        raise ValueError(\"No valid label column found. Expecting 'label_binary' or integer 'label'.\")\n",
    "\n",
    "    # ===== Step 1.5: Ensure Labels Are Integer Type ===== #\n",
    "    ds = ds.map(lambda x: {\"labels\": int(x[\"labels\"])})\n",
    "\n",
    "\n",
    "    # ===== Step 2: Remove 'label' column if it’s a string (textual label, not numeric) ===== #\n",
    "    if \"label\" in ds.column_names and \"labels\" in ds.column_names:\n",
    "        if isinstance(ds[\"label\"][0], str):\n",
    "            ds = ds.remove_columns(\"label\")\n",
    "\n",
    "    # ===== Step 3: Apply Tokenizer ===== #\n",
    "    ds = ds.map(tokenize, batched=True)\n",
    "\n",
    "    # Step 4: Keep only model-relevant fields\n",
    "    model_inputs = {\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"}\n",
    "    to_remove = [col for col in ds.column_names if col not in model_inputs]\n",
    "    ds = ds.remove_columns(to_remove)\n",
    "\n",
    "    # debug check\n",
    "    print(\"Final columns after cleaning:\", ds.column_names)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:02.604683Z",
     "iopub.status.busy": "2025-07-15T05:19:02.604436Z",
     "iopub.status.idle": "2025-07-15T05:19:14.139078Z",
     "shell.execute_reply": "2025-07-15T05:19:14.138019Z",
     "shell.execute_reply.started": "2025-07-15T05:19:02.604668Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89b2d62fe134076a160e542bed86d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cec25f964740e59568d18753474a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns after cleaning: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a92c10039a40a7b5e4f02360a4095a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971fa4bca4564078be2755d160625de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns after cleaning: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423a475b89db469bbbaf4157ac7847a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f6f5d9b42241128f6f1750da8428b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns after cleaning: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Final dataset columns: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "\n",
      "Sample 0:\n",
      "{'labels': 1, 'input_ids': [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2678, 2706, 190, 50, 2383, 1779, 1091, 20, 594, 316, 1987, 7672, 254, 14043, 9743, 31791, 23, 9, 4, 3], 'token_type_ids': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Sample 1:\n",
      "{'labels': 0, 'input_ids': [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 67, 648, 19, 18, 4245, 27, 102, 1376, 29, 9363, 2124, 47, 1228, 36, 34, 17, 12, 152, 1966, 741, 9498, 773, 9, 12, 315, 20, 848, 13, 16264, 1235, 968, 7051, 13490, 1204, 1235, 4112, 1126, 66, 2151, 10687, 29, 7084, 11619, 417, 13, 815, 6844, 2977, 23, 21, 18, 1735, 20, 3851, 154, 47, 687, 22, 383, 13751, 25, 160, 142, 20, 211, 985, 120, 1166, 21, 120, 12247, 4, 3], 'token_type_ids': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Sample 2:\n",
      "{'labels': 0, 'input_ids': [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 17, 10, 13408, 2463, 11, 4, 3], 'token_type_ids': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# ===== Apply Processing Pipeline to Each Split ===== #\n",
    "train_ds = tokenize_and_clean(train)\n",
    "val_ds   = tokenize_and_clean(val)\n",
    "test_ds  = tokenize_and_clean(test)\n",
    "\n",
    "print(\"Final dataset columns:\", train_ds.column_names) # Only 'input_ids', 'attention_mask', 'labels'\n",
    "\n",
    "# Inspect first few samples\n",
    "for i in range(3):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:14.140292Z",
     "iopub.status.busy": "2025-07-15T05:19:14.140029Z",
     "iopub.status.idle": "2025-07-15T05:19:18.336660Z",
     "shell.execute_reply": "2025-07-15T05:19:18.335941Z",
     "shell.execute_reply.started": "2025-07-15T05:19:14.140274Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b110e3ff64b4150942565a94a585af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ===== Initialize Model ===== #\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:18.337715Z",
     "iopub.status.busy": "2025-07-15T05:19:18.337411Z",
     "iopub.status.idle": "2025-07-15T05:19:18.347913Z",
     "shell.execute_reply": "2025-07-15T05:19:18.344506Z",
     "shell.execute_reply.started": "2025-07-15T05:19:18.337695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "\n",
    "    y_true = pred.label_ids # The true labels of the test set\n",
    "    y_pred = pred.predictions.argmax(-1) # The model's predicted class (the one with the highest probability)\n",
    "\n",
    "    # Calculate evaluation metrics for class 0\n",
    "    precision = precision_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "\n",
    "    # Overall accuracy of the model\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate Weighted metrics\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision_weighted = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    recall_weighted = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    # Calculate micro-averaged precision, recall, and F1 (global average across all classes)\n",
    "    precision_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    recall_micro = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    # Macro metrics\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # Return all metrics in a dictionary\n",
    "    return {\n",
    "        'eval_f1': f1_weighted,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_weighted\": precision_weighted,\n",
    "        \"recall_weighted\": recall_weighted,\n",
    "\n",
    "        \"support_1\": int((y_true == 1).sum()),  # Number of class 1 samples\n",
    "        \"support_0\": int((y_true == 0).sum())  # Number of class 0 samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:18.349572Z",
     "iopub.status.busy": "2025-07-15T05:19:18.349316Z",
     "iopub.status.idle": "2025-07-15T05:19:18.430670Z",
     "shell.execute_reply": "2025-07-15T05:19:18.429955Z",
     "shell.execute_reply.started": "2025-07-15T05:19:18.349555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Define Training Arguments ===== #\n",
    "# Key parameters for reproducibility and fine-tuning control\n",
    "# This block defines how training should proceed, how models are saved, and evaluation behavior\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,  # Folder to store model checkpoints and logs\n",
    "    # evaluation_strategy='epoch',  # Evaluate model after each epoch\n",
    "    eval_strategy='epoch', \n",
    "    save_strategy='epoch', # Save model after each epoch\n",
    "    save_total_limit=1, # Keep only the latest saved model to reduce disk usage\n",
    "    # learning_rate=2e-5,  # Initial learning rate for optimizer\n",
    "    # learning_rate=3e-5, # Alternative setting for grid search\n",
    "    learning_rate=5e-5, # # Max learning rate\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1, \n",
    "    per_device_train_batch_size=8,  # Number of samples per GPU/CPU during training\n",
    "    per_device_eval_batch_size=8,  # Number of samples per GPU/CPU during validation\n",
    "    num_train_epochs=100, # Max number of training epochs (early stopping can interrupt before 100)\n",
    "    weight_decay=0.01, # L2 regularization to reduce overfitting\n",
    "    load_best_model_at_end=True,  # Restore best model at the end of training based on eval metric\n",
    "    metric_for_best_model = 'eval_f1',\n",
    "    greater_is_better = True,\n",
    "    logging_dir=f\"logs/{run_id}\", # Directory to save training logs\n",
    "    logging_strategy='epoch', # Log metrics at the end of each epoch\n",
    "    seed=42, # Set random seed for reproducibility\n",
    "    fp16=True,  # Use mixed precision for faster training on supported GPUs\n",
    "    remove_unused_columns=False,  # Retain all columns to prevent loss of important features\n",
    "    report_to='none', # Do not report metrics to third-party tools (e.g., WandB)\n",
    "    disable_tqdm=True,  # Disable tqdm progress bar to reduce console clutter in Colab\n",
    "    eval_accumulation_steps=32,  # Accumulate gradients during evaluation to save memory\n",
    "    no_cuda=False  # Use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:18.431754Z",
     "iopub.status.busy": "2025-07-15T05:19:18.431511Z",
     "iopub.status.idle": "2025-07-15T05:19:18.437592Z",
     "shell.execute_reply": "2025-07-15T05:19:18.436822Z",
     "shell.execute_reply.started": "2025-07-15T05:19:18.431738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Custom Logging Callback for Per-Epoch Metrics ===== #\n",
    "# This callback logs evaluation metrics (loss, accuracy, precision, recall, F1) after every epoch\n",
    "# It writes the results into a CSV file \"epoch_log.csv\" for visualization or debugging\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "import csv\n",
    "\n",
    "class MetricsLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, log_file=output_dir / \"epoch_log.csv\"):\n",
    "        self.log_file = log_file\n",
    "        with open(self.log_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                \"epoch\", \"learning_rate\",\n",
    "                \"eval_loss\", \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
    "            ])\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None or \"eval_loss\" not in logs:\n",
    "            return  # Skip non-eval logs\n",
    "\n",
    "        current_lr = logs.get(\"learning_rate\", None)\n",
    "\n",
    "        with open(self.log_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                round(state.epoch, 2),\n",
    "                current_lr,\n",
    "                logs.get(\"eval_loss\"),\n",
    "                logs.get(\"eval_accuracy\"),\n",
    "                logs.get(\"eval_precision\"),\n",
    "                logs.get(\"eval_recall\"),\n",
    "                logs.get(\"eval_f1\")\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:18.438802Z",
     "iopub.status.busy": "2025-07-15T05:19:18.438353Z",
     "iopub.status.idle": "2025-07-15T05:19:18.462977Z",
     "shell.execute_reply": "2025-07-15T05:19:18.462422Z",
     "shell.execute_reply.started": "2025-07-15T05:19:18.438779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===== Data Collator for Padding =====\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:18.463824Z",
     "iopub.status.busy": "2025-07-15T05:19:18.463610Z",
     "iopub.status.idle": "2025-07-15T05:19:18.634172Z",
     "shell.execute_reply": "2025-07-15T05:19:18.633287Z",
     "shell.execute_reply.started": "2025-07-15T05:19:18.463800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f24252512614d109c14e68c19b9c07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ===== Custom Trainer with Class Weights =====\n",
    "#  Enables weighted loss to address label imbalance in binary classification\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Use weighted cross entropy loss\n",
    "        #loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        #loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
    "        \n",
    "        num_labels = model.module.config.num_labels if hasattr(model, \"module\") else model.config.num_labels\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "\n",
    "        # loss = loss_fct(logits.view(-1, model.module.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Instantiate Trainer with early stopping and epoch logger\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5), MetricsLoggerCallback()],\n",
    "    data_collator=data_collator  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:18.635634Z",
     "iopub.status.busy": "2025-07-15T05:19:18.635248Z",
     "iopub.status.idle": "2025-07-15T05:19:18.641649Z",
     "shell.execute_reply": "2025-07-15T05:19:18.640658Z",
     "shell.execute_reply.started": "2025-07-15T05:19:18.635610Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': 1, 'input_ids': [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2678, 2706, 190, 50, 2383, 1779, 1091, 20, 594, 316, 1987, 7672, 254, 14043, 9743, 31791, 23, 9, 4, 3], 'token_type_ids': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "'statement' field has been removed after tokenization (expected).\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of a problematic sample\n",
    "sample = train_ds[0]\n",
    "print(sample)\n",
    "\n",
    "# Check if 'statement' still exists before accessing\n",
    "if 'statement' in sample:\n",
    "    print(\"Statement type:\", type(sample['statement']))\n",
    "    print(\"Statement value:\", sample['statement'])\n",
    "else:\n",
    "    print(\"'statement' field has been removed after tokenization (expected).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T05:19:18.642729Z",
     "iopub.status.busy": "2025-07-15T05:19:18.642483Z",
     "iopub.status.idle": "2025-07-15T10:05:03.238559Z",
     "shell.execute_reply": "2025-07-15T10:05:03.237952Z",
     "shell.execute_reply.started": "2025-07-15T05:19:18.642712Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3401, 'grad_norm': 0.10667973011732101, 'learning_rate': 4.998062766369624e-06, 'epoch': 1.0}\n",
      "{'eval_f1': 0.9581661151569278, 'eval_loss': 0.5583905577659607, 'eval_accuracy': 0.9577966101694916, 'eval_precision': 0.9833457618692518, 'eval_recall': 0.9560173997100049, 'eval_precision_micro': 0.9577966101694916, 'eval_recall_micro': 0.9577966101694916, 'eval_f1_micro': 0.9577966101694916, 'eval_precision_macro': 0.943191261328872, 'eval_recall_macro': 0.9589962140434247, 'eval_f1_macro': 0.9505318208078093, 'eval_f1_weighted': 0.9581661151569278, 'eval_precision_weighted': 0.9593619551058115, 'eval_recall_weighted': 0.9577966101694916, 'eval_support_1': 1762, 'eval_support_0': 4138, 'eval_runtime': 59.1795, 'eval_samples_per_second': 99.697, 'eval_steps_per_second': 6.235, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3845, 'grad_norm': 0.05664745345711708, 'learning_rate': 9.998062766369625e-06, 'epoch': 2.0}\n",
      "{'eval_f1': 0.9658732009260265, 'eval_loss': 0.5646498203277588, 'eval_accuracy': 0.9659322033898305, 'eval_precision': 0.9733108920413561, 'eval_recall': 0.9782503624939585, 'eval_precision_micro': 0.9659322033898305, 'eval_recall_micro': 0.9659322033898305, 'eval_f1_micro': 0.9659322033898305, 'eval_precision_macro': 0.9608082317759912, 'eval_recall_macro': 0.9576268838576489, 'eval_f1_macro': 0.959197493592761, 'eval_f1_weighted': 0.9658732009260265, 'eval_precision_weighted': 0.9658432014015008, 'eval_recall_weighted': 0.9659322033898305, 'eval_support_1': 1762, 'eval_support_0': 4138, 'eval_runtime': 58.959, 'eval_samples_per_second': 100.069, 'eval_steps_per_second': 6.259, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.289, 'grad_norm': 1.6619789600372314, 'learning_rate': 1.4998062766369624e-05, 'epoch': 3.0}\n",
      "{'eval_f1': 0.9654400058154613, 'eval_loss': 0.6373733878135681, 'eval_accuracy': 0.9655932203389831, 'eval_precision': 0.9694583631591506, 'eval_recall': 0.9818753020782987, 'eval_precision_micro': 0.9655932203389831, 'eval_recall_micro': 0.9655932203389831, 'eval_f1_micro': 0.9655932203389831, 'eval_precision_macro': 0.962786525055292, 'eval_recall_macro': 0.9546152900856874, 'eval_f1_macro': 0.9585713698175774, 'eval_f1_weighted': 0.9654400058154613, 'eval_precision_weighted': 0.9654733534171172, 'eval_recall_weighted': 0.9655932203389831, 'eval_support_1': 1762, 'eval_support_0': 4138, 'eval_runtime': 59.0168, 'eval_samples_per_second': 99.972, 'eval_steps_per_second': 6.252, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2299, 'grad_norm': 0.03739162161946297, 'learning_rate': 1.9998062766369624e-05, 'epoch': 4.0}\n",
      "{'eval_f1': 0.9660538499081652, 'eval_loss': 0.545914888381958, 'eval_accuracy': 0.9659322033898305, 'eval_precision': 0.9809430735401906, 'eval_recall': 0.9702754954084098, 'eval_precision_micro': 0.9659322033898305, 'eval_recall_micro': 0.9659322033898305, 'eval_f1_micro': 0.9659322033898305, 'eval_precision_macro': 0.956437225757367, 'eval_recall_macro': 0.963003808998189, 'eval_f1_macro': 0.9596309137400894, 'eval_f1_weighted': 0.9660538499081652, 'eval_precision_weighted': 0.9663060214068566, 'eval_recall_weighted': 0.9659322033898305, 'eval_support_1': 1762, 'eval_support_0': 4138, 'eval_runtime': 59.0375, 'eval_samples_per_second': 99.937, 'eval_steps_per_second': 6.25, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1722, 'grad_norm': 0.07977437973022461, 'learning_rate': 2.4998062766369627e-05, 'epoch': 5.0}\n",
      "{'eval_f1': 0.9689550759377401, 'eval_loss': 0.5903756618499756, 'eval_accuracy': 0.9689830508474576, 'eval_precision': 0.9766208724993974, 'eval_recall': 0.9792170130497825, 'eval_precision_micro': 0.9689830508474576, 'eval_recall_micro': 0.9689830508474576, 'eval_f1_micro': 0.9689830508474576, 'eval_precision_macro': 0.9637530404758552, 'eval_recall_macro': 0.9620829673648459, 'eval_f1_macro': 0.9629124954387727, 'eval_f1_weighted': 0.9689550759377401, 'eval_precision_weighted': 0.9689350691009292, 'eval_recall_weighted': 0.9689830508474576, 'eval_support_1': 1762, 'eval_support_0': 4138, 'eval_runtime': 58.9934, 'eval_samples_per_second': 100.011, 'eval_steps_per_second': 6.255, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1574, 'grad_norm': 1.2721647024154663, 'learning_rate': 2.9998062766369626e-05, 'epoch': 6.0}\n",
      "{'eval_f1': 0.9654461587827976, 'eval_loss': 0.6480091214179993, 'eval_accuracy': 0.9654237288135593, 'eval_precision': 0.976271186440678, 'eval_recall': 0.9743837602706622, 'eval_precision_micro': 0.9654237288135593, 'eval_recall_micro': 0.9654237288135593, 'eval_f1_micro': 0.9654237288135593, 'eval_precision_macro': 0.9581920903954803, 'eval_recall_macro': 0.9593825725303368, 'eval_f1_macro': 0.9587844579190548, 'eval_f1_weighted': 0.9654461587827976, 'eval_precision_weighted': 0.965472756870631, 'eval_recall_weighted': 0.9654237288135593, 'eval_support_1': 1762, 'eval_support_0': 4138, 'eval_runtime': 58.8842, 'eval_samples_per_second': 100.197, 'eval_steps_per_second': 6.267, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1573, 'grad_norm': 0.05540476739406586, 'learning_rate': 3.499806276636963e-05, 'epoch': 7.0}\n",
      "{'eval_f1': 0.9621880498538661, 'eval_loss': 0.8065259456634521, 'eval_accuracy': 0.9625423728813559, 'eval_precision': 0.961149046385684, 'eval_recall': 0.9864668922184631, 'eval_precision_micro': 0.9625423728813559, 'eval_recall_micro': 0.9625423728813559, 'eval_f1_micro': 0.9625423728813559, 'eval_precision_macro': 0.9636356242212751, 'eval_recall_macro': 0.9464116526926595, 'eval_f1_macro': 0.954464457988582, 'eval_f1_weighted': 0.9621880498538661, 'eval_precision_weighted': 0.9626342498251116, 'eval_recall_weighted': 0.9625423728813559, 'eval_support_1': 1762, 'eval_support_0': 4138, 'eval_runtime': 58.9389, 'eval_samples_per_second': 100.104, 'eval_steps_per_second': 6.261, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1981, 'grad_norm': 0.31435084342956543, 'learning_rate': 3.999806276636963e-05, 'epoch': 8.0}\n",
      "{'eval_f1': 0.9685847078709224, 'eval_loss': 0.4878431260585785, 'eval_accuracy': 0.9684745762711865, 'eval_precision': 0.982657547630679, 'eval_recall': 0.972208796520058, 'eval_precision_micro': 0.9684745762711865, 'eval_recall_micro': 0.9684745762711865, 'eval_f1_micro': 0.9684745762711865, 'eval_precision_macro': 0.9594904570933018, 'eval_recall_macro': 0.9659568386686556, 'eval_f1_macro': 0.962637601485181, 'eval_f1_weighted': 0.9685847078709224, 'eval_precision_weighted': 0.9688201193164898, 'eval_recall_weighted': 0.9684745762711865, 'eval_support_1': 1762, 'eval_support_0': 4138, 'eval_runtime': 59.019, 'eval_samples_per_second': 99.968, 'eval_steps_per_second': 6.252, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4655, 'grad_norm': 1611603.875, 'learning_rate': 4.499806276636963e-05, 'epoch': 9.0}\n",
      "{'eval_f1': 0.13735600299076667, 'eval_loss': 0.746648907661438, 'eval_accuracy': 0.2986440677966102, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_precision_micro': 0.2986440677966102, 'eval_recall_micro': 0.2986440677966102, 'eval_f1_micro': 0.2986440677966102, 'eval_precision_macro': 0.1493220338983051, 'eval_recall_macro': 0.5, 'eval_f1_macro': 0.22996606630122682, 'eval_f1_weighted': 0.13735600299076667, 'eval_precision_weighted': 0.08918827923010629, 'eval_recall_weighted': 0.2986440677966102, 'eval_support_1': 1762, 'eval_support_0': 4138, 'eval_runtime': 58.8856, 'eval_samples_per_second': 100.194, 'eval_steps_per_second': 6.266, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7029, 'grad_norm': 1061809.125, 'learning_rate': 4.999806276636962e-05, 'epoch': 10.0}\n",
      "{'eval_f1': 0.5782448391029342, 'eval_loss': 0.6940978169441223, 'eval_accuracy': 0.7013559322033899, 'eval_precision': 0.7013559322033899, 'eval_recall': 1.0, 'eval_precision_micro': 0.7013559322033899, 'eval_recall_micro': 0.7013559322033899, 'eval_f1_micro': 0.7013559322033899, 'eval_precision_macro': 0.35067796610169494, 'eval_recall_macro': 0.5, 'eval_f1_macro': 0.4122335126519227, 'eval_f1_weighted': 0.5782448391029342, 'eval_precision_weighted': 0.49190014363688594, 'eval_recall_weighted': 0.7013559322033899, 'eval_support_1': 1762, 'eval_support_0': 4138, 'eval_runtime': 58.9899, 'eval_samples_per_second': 100.017, 'eval_steps_per_second': 6.255, 'epoch': 10.0}\n",
      "{'train_runtime': 17143.8443, 'train_samples_per_second': 240.798, 'train_steps_per_second': 15.055, 'train_loss': 0.3096932270415113, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# ===== Train Model with Timer ===== #\n",
    "# Record how long training took\n",
    "start_time = time()\n",
    "trainer.train()\n",
    "end_time = time()\n",
    "train_time = round(end_time - start_time, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.121921Z",
     "iopub.status.idle": "2025-07-15T10:07:10.122215Z",
     "shell.execute_reply": "2025-07-15T10:07:10.122079Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.122066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.123029Z",
     "iopub.status.idle": "2025-07-15T10:07:10.123248Z",
     "shell.execute_reply": "2025-07-15T10:07:10.123157Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.123148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -lt /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_2025071*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T10:26:47.225280Z",
     "iopub.status.busy": "2025-07-15T10:26:47.224677Z",
     "iopub.status.idle": "2025-07-15T10:26:47.230567Z",
     "shell.execute_reply": "2025-07-15T10:26:47.230013Z",
     "shell.execute_reply.started": "2025-07-15T10:26:47.225258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Save training loss history (for reproducibility / visualization)\n",
    "with open(output_dir / \"train_loss_curve.json\", \"w\") as f:\n",
    "    json.dump(trainer.state.log_history, f, indent=2)\n",
    "\n",
    "# ===== Save training time ===== #\n",
    "with open(output_dir / \"training_time_log.json\", \"w\") as f:\n",
    "    json.dump({\"train_time_seconds\": train_time}, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-07-15T10:05:03.246134Z",
     "iopub.status.busy": "2025-07-15T10:05:03.245884Z",
     "iopub.status.idle": "2025-07-15T10:05:03.273237Z",
     "shell.execute_reply": "2025-07-15T10:05:03.272692Z",
     "shell.execute_reply.started": "2025-07-15T10:05:03.246110Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: transformer, current LR: 5e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model loaded from: {trainer.model.base_model_prefix}, current LR: {trainer.optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T10:05:03.274160Z",
     "iopub.status.busy": "2025-07-15T10:05:03.273989Z",
     "iopub.status.idle": "2025-07-15T10:07:09.826108Z",
     "shell.execute_reply": "2025-07-15T10:07:09.825511Z",
     "shell.execute_reply.started": "2025-07-15T10:05:03.274147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始手动预测测试集...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d7e4cafc804ed5956185ccca535b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "手动预测进度:   0%|          | 0/1475 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动预测完成。\n",
      "✅ prediction.csv 已包含所有列 (statement, true_label, pred_label, logits, model_name, run_id)，并保存至 /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900/prediction.csv\n",
      "✅ classification_report.json 已保存至 /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900/classification_report.json\n",
      "✅ confusion_matrix.json 已保存至 /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900/confusion_matrix.json\n",
      "✅ eval_metrics.json 已保存至 /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900/eval_metrics.json\n",
      "✅ train_loss_curve.json 已保存至 /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900/train_loss_curve.json\n",
      "✅ training_time_log.json 已保存至 /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900/training_time_log.json\n",
      "Final learning rate (from best checkpoint): 5e-05\n",
      "Early stopped at epoch: 10\n",
      "✅ best_lr.txt 已保存至 /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900/best_lr.txt\n",
      "✅ config.json 已保存至 /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900/config.json\n",
      "✅ README.txt 已保存至 /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900/README.txt\n",
      "✅ training_args.json 已保存至 /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900/training_args.json\n",
      "🎉 所有结果已保存至总目录: /kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250715_051900\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "from tqdm.auto import tqdm # 确保 tqdm 在 Notebook 中显示\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import json\n",
    "\n",
    "print(\"开始手动预测测试集...\")\n",
    "\n",
    "# 获取在训练结束时加载的最佳模型\n",
    "# trainer.model 此时应该已经加载了最佳权重（因为 training_args 中设置了 load_best_model_at_end=True）\n",
    "model_to_predict = trainer.model\n",
    "model_to_predict.eval() # 将模型设置为评估模式\n",
    "# 确保模型在正确的设备上（GPU 如果可用）\n",
    "model_to_predict.to(device) # 'device' 变量在 Cell 3 中定义了\n",
    "\n",
    "# 使用 DataCollatorWithPadding 确保测试数据批次正确填充\n",
    "# return_tensors=\"pt\" 确保返回 PyTorch 张量\n",
    "data_collator_for_prediction = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=trainer.args.per_device_eval_batch_size, # 使用训练参数中定义的评估批次大小\n",
    "    collate_fn=data_collator_for_prediction,\n",
    "    shuffle=False # 预测时不打乱数据顺序\n",
    ")\n",
    "\n",
    "all_logits = []\n",
    "all_true_labels = []\n",
    "# 从原始的 'test' DataFrame 中获取对应的 'statement'\n",
    "# 注意：test_ds 是 tokenized 后的数据集，可能移除了原始列\n",
    "# 我们需要从原始的 'test' DataFrame 中提取文本，并确保顺序一致\n",
    "original_statements = test['statement'].tolist() # 'test' DataFrame 在 Cell 8 中定义\n",
    "\n",
    "with torch.no_grad(): # 在预测过程中禁用梯度计算，节省内存和时间\n",
    "    for batch_idx, batch in enumerate(tqdm(test_dataloader, desc=\"手动预测进度\")):\n",
    "        # 将输入数据移动到模型所在的设备\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask', 'token_type_ids']}\n",
    "\n",
    "        # 执行模型前向传播\n",
    "        outputs = model_to_predict(**inputs)\n",
    "\n",
    "        # 从模型输出中提取 logits，并移回 CPU\n",
    "        logits = outputs.logits.cpu()\n",
    "        all_logits.append(logits)\n",
    "\n",
    "        # 收集真实标签 (如果批次中有标签)\n",
    "        if 'labels' in batch:\n",
    "            all_true_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "# 合并所有批次的 logits\n",
    "final_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "# 根据 logits 确定预测标签\n",
    "final_preds_labels = np.argmax(final_logits, axis=-1)\n",
    "\n",
    "print(\"手动预测完成。\")\n",
    "\n",
    "# ===== 保存原始预测表 (prediction.csv) ===== #\n",
    "# 确保 'test' DataFrame 和 test_ds 的样本顺序是匹配的\n",
    "# 否则需要一个更复杂的映射机制\n",
    "# 由于您的数据加载和分词流程通常会保持顺序，这里假设它们是匹配的。\n",
    "results_df = pd.DataFrame({\n",
    "    'statement': original_statements, # 使用从原始 DataFrame 提取的 statement\n",
    "    'true_label': all_true_labels, # 使用手动收集的真实标签\n",
    "    'pred_label': final_preds_labels,\n",
    "    'logits': final_logits.tolist(), # 将 logits 转换为列表形式以便保存到 CSV\n",
    "    'model_name': model_name,\n",
    "    'run_id': run_id\n",
    "})\n",
    "results_df.to_csv(output_dir / \"prediction.csv\", index=False)\n",
    "print(f\"✅ prediction.csv 已包含所有列 (statement, true_label, pred_label, logits, model_name, run_id)，并保存至 {output_dir / 'prediction.csv'}\")\n",
    "\n",
    "\n",
    "# ===== 计算并保存评估指标 ===== #\n",
    "# 使用手动预测的结果计算分类报告和混淆矩阵\n",
    "conf_matrix = confusion_matrix(all_true_labels, final_preds_labels)\n",
    "report = classification_report(all_true_labels, final_preds_labels, output_dict=True, zero_division=0)\n",
    "\n",
    "with open(output_dir / \"classification_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(f\"✅ classification_report.json 已保存至 {output_dir / 'classification_report.json'}\")\n",
    "\n",
    "with open(output_dir / \"confusion_matrix.json\", \"w\") as f:\n",
    "    json.dump(conf_matrix.tolist(), f, indent=2)\n",
    "print(f\"✅ confusion_matrix.json 已保存至 {output_dir / 'confusion_matrix.json'}\")\n",
    "\n",
    "# 将 eval_metrics 也保存到 json 文件\n",
    "# 确保这里的 metrics 结构与 compute_metrics 函数的输出保持一致\n",
    "# 这里我们手动构造 preds_for_metrics 来兼容 compute_metrics 函数\n",
    "class ManualPredictionOutput:\n",
    "    def __init__(self, label_ids, predictions):\n",
    "        self.label_ids = label_ids\n",
    "        self.predictions = predictions\n",
    "manual_preds_for_metrics = ManualPredictionOutput(np.array(all_true_labels), final_logits)\n",
    "eval_metrics = compute_metrics(manual_preds_for_metrics)\n",
    "\n",
    "with open(output_dir / \"eval_metrics.json\", \"w\") as f:\n",
    "    json.dump(eval_metrics, f, indent=2)\n",
    "print(f\"✅ eval_metrics.json 已保存至 {output_dir / 'eval_metrics.json'}\")\n",
    "\n",
    "# ===== 保存训练时间和学习率等配置信息 (原 Cell 25, 33, 34) ===== #\n",
    "# 这部分保持与原有 Notebook 中一致，只是确保它们在手动预测之后运行\n",
    "\n",
    "# 保存训练损失历史\n",
    "with open(output_dir / \"train_loss_curve.json\", \"w\") as f:\n",
    "    json.dump(trainer.state.log_history, f, indent=2)\n",
    "print(f\"✅ train_loss_curve.json 已保存至 {output_dir / 'train_loss_curve.json'}\")\n",
    "\n",
    "# 保存训练时间\n",
    "# train_time 变量应该在 trainer.train() 之后被设置\n",
    "with open(output_dir / \"training_time_log.json\", \"w\") as f:\n",
    "    json.dump({\"train_time_seconds\": train_time}, f, indent=2)\n",
    "print(f\"✅ training_time_log.json 已保存至 {output_dir / 'training_time_log.json'}\")\n",
    "\n",
    "# 估计学习率 & 早停 epoch\n",
    "best_lr = trainer.optimizer.param_groups[0]['lr'] # 从 Trainer 获取最终学习率\n",
    "# 这里我们直接使用 trainer.state.epoch 来获取早停时的 epoch，因为它更准确\n",
    "epoch_stopped_at = trainer.state.epoch\n",
    "\n",
    "print(f\"Final learning rate (from best checkpoint): {best_lr}\")\n",
    "print(f\"Early stopped at epoch: {int(epoch_stopped_at)}\")\n",
    "\n",
    "with open(output_dir / \"best_lr.txt\", \"w\") as f:\n",
    "    f.write(str(best_lr))\n",
    "print(f\"✅ best_lr.txt 已保存至 {output_dir / 'best_lr.txt'}\")\n",
    "\n",
    "# 保存模型 & 训练配置快照\n",
    "config_snapshot = {\n",
    "    \"model_name\": model_name,\n",
    "    \"run_id\": run_id,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n",
    "    \"warmup_ratio\": training_args.warmup_ratio,\n",
    "    \"batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"num_epochs\": training_args.num_train_epochs,\n",
    "    \"weight_decay\": training_args.weight_decay,\n",
    "    \"early_stopping\": True,\n",
    "    \"best_lr\": best_lr,\n",
    "    \"epoch_stopped_at\": int(epoch_stopped_at),\n",
    "    \"class_weights\": class_weights.cpu().tolist(),\n",
    "    \"datetime\": timestamp\n",
    "}\n",
    "with open(output_dir / \"config.json\", \"w\") as f:\n",
    "    json.dump(config_snapshot, f, indent=2)\n",
    "print(f\"✅ config.json 已保存至 {output_dir / 'config.json'}\")\n",
    "\n",
    "# 写入可读的运行信息\n",
    "with open(output_dir / \"README.txt\", \"w\") as f:\n",
    "    f.write(f\"Run ID: {run_id}\\nModel: {model_name}\\nCreated: {timestamp}\\n\")\n",
    "    f.write(f\"Best LR (from checkpoint): {best_lr}\\nStopped at epoch: {int(epoch_stopped_at)}\\n\")\n",
    "print(f\"✅ README.txt 已保存至 {output_dir / 'README.txt'}\")\n",
    "\n",
    "# 保存 TrainingArguments Params (原 Cell 35 开头部分)\n",
    "with open(output_dir / \"training_args.json\", \"w\") as f:\n",
    "    json.dump(training_args.to_dict(), f, indent=2)\n",
    "print(f\"✅ training_args.json 已保存至 {output_dir / 'training_args.json'}\")\n",
    "\n",
    "print(f\"🎉 所有结果已保存至总目录: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T10:31:03.201686Z",
     "iopub.status.busy": "2025-07-15T10:31:03.201403Z",
     "iopub.status.idle": "2025-07-15T10:31:03.289367Z",
     "shell.execute_reply": "2025-07-15T10:31:03.288646Z",
     "shell.execute_reply.started": "2025-07-15T10:31:03.201669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型评估总表已更新并保存至：/kaggle/working/MSc_Claim_Experiment/outputs/results/final_model_comparison.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ===== Append Main Metrics to Excel Summary Table ===== #\n",
    "# 创建/更新一个 Excel 总结表格，用于最终模型比较\n",
    "summary_path = project_root / \"outputs\" / \"results\" / \"final_model_comparison.xlsx\"\n",
    "\n",
    "# 构建一行 summary 数据，使用手动预测的结果\n",
    "summary_row = {\n",
    "    \"model_name\": model_name,\n",
    "    \"run_id\": run_id,\n",
    "    \"accuracy\": report[\"accuracy\"], # 使用前面计算的报告中的准确率\n",
    "\n",
    "    # Per-class metrics\n",
    "    \"precision_0\": report['0']['precision'],\n",
    "    \"recall_0\": report['0']['recall'],\n",
    "    \"f1_0\": report['0']['f1-score'],\n",
    "    \"precision_1\": report['1']['precision'],\n",
    "    \"recall_1\": report['1']['recall'],\n",
    "    \"f1_1\": report['1']['f1-score'],\n",
    "\n",
    "    # Micro-average (使用手动计算的值)\n",
    "    \"precision_micro\": precision_score(all_true_labels, final_preds_labels, average='micro', zero_division=0),\n",
    "    \"recall_micro\": recall_score(all_true_labels, final_preds_labels, average='micro', zero_division=0),\n",
    "    \"f1_micro\": f1_score(all_true_labels, final_preds_labels, average='micro', zero_division=0),\n",
    "\n",
    "    # Macro-average\n",
    "    \"precision_macro\": report['macro avg']['precision'],\n",
    "    \"recall_macro\": report['macro avg']['recall'],\n",
    "    \"f1_macro\": report['macro avg']['f1-score'],\n",
    "\n",
    "    # Weighted Averages\n",
    "    \"precision_weighted\": report['weighted avg']['precision'],\n",
    "    \"recall_weighted\": report['weighted avg']['recall'],\n",
    "    \"f1_weighted\": report['weighted avg']['f1-score'],\n",
    "\n",
    "    \"lr_max\": training_args.learning_rate,\n",
    "    \"lr_min\": 0.0,\n",
    "    \"best_lr\": best_lr, # 使用上面更新的 best_lr\n",
    "    \"epoch_stopped_at\": int(epoch_stopped_at), # 使用上面更新的 epoch_stopped_at\n",
    "\n",
    "    # Class balance info\n",
    "    \"support_1\": report['1']['support'],\n",
    "    \"support_0\": report['0']['support'],\n",
    "    \"train_time_sec\": train_time # 使用前面计算的 train_time\n",
    "}\n",
    "\n",
    "# Excel Integration\n",
    "df_row = pd.DataFrame([summary_row])\n",
    "if not summary_path.exists():\n",
    "    df_row.to_excel(summary_path, index=False)\n",
    "else:\n",
    "    existing = pd.read_excel(summary_path)\n",
    "    updated = pd.concat([existing, df_row], ignore_index=True)\n",
    "    updated.to_excel(summary_path, index=False)\n",
    "\n",
    "print(f\"✅ 模型评估总表已更新并保存至：{summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.087429Z",
     "iopub.status.idle": "2025-07-15T10:07:10.087666Z",
     "shell.execute_reply": "2025-07-15T10:07:10.087561Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.087552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 提前构造路径\n",
    "model_path = project_root / \"outputs\" / \"results\" / \"saved_models\" / f\"{model_name.replace('/', '_')}_{run_id}\"\n",
    "\n",
    "# 打印输出\n",
    "print(f\"✅ 最佳模型权重已保存至: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T10:33:32.700927Z",
     "iopub.status.busy": "2025-07-15T10:33:32.700346Z",
     "iopub.status.idle": "2025-07-15T10:34:40.000025Z",
     "shell.execute_reply": "2025-07-15T10:34:39.999391Z",
     "shell.execute_reply.started": "2025-07-15T10:33:32.700901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/results_backup.zip'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Backup all output files (including intermediate files, models, logs, etc.)\n",
    "# This creates a full archive of the experiment outputs\n",
    "shutil.make_archive(\n",
    "    \"/kaggle/working/results_backup\", # 打包后的zip文件路径和名称\n",
    "    'zip',                             # 压缩格式\n",
    "    \"/kaggle/working/MSc_Claim_Experiment/outputs\" # 要打包的源目录\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.090501Z",
     "iopub.status.idle": "2025-07-15T10:07:10.090837Z",
     "shell.execute_reply": "2025-07-15T10:07:10.090687Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.090672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.097138Z",
     "iopub.status.idle": "2025-07-15T10:07:10.097494Z",
     "shell.execute_reply": "2025-07-15T10:07:10.097319Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.097303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# ===== Predict and Save Outputs ===== #\n",
    "# Generate predictions from best model on the test set\n",
    "preds = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.098302Z",
     "iopub.status.idle": "2025-07-15T10:07:10.098583Z",
     "shell.execute_reply": "2025-07-15T10:07:10.098475Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.098462Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the best model weights for reproducibility or future use\n",
    "trainer.save_model(project_root / \"outputs\" / \"results\" / \"saved_models\" / f\"{model_name.replace('/', '_')}_{run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.099621Z",
     "iopub.status.idle": "2025-07-15T10:07:10.099881Z",
     "shell.execute_reply": "2025-07-15T10:07:10.099782Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.099770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract predicted labels\n",
    "preds_labels = np.argmax(preds.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.100516Z",
     "iopub.status.idle": "2025-07-15T10:07:10.100745Z",
     "shell.execute_reply": "2025-07-15T10:07:10.100647Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.100635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Save Raw Prediction Table ===== #\n",
    "# Save full test set results with input, true label, predicted label, and logits\n",
    "results = pd.DataFrame({\n",
    "    'statement': test['statement'].reset_index(drop=True),\n",
    "    'true_label': test['label_binary'].reset_index(drop=True),\n",
    "    'pred_label': preds_labels,\n",
    "    'logits': preds.predictions.tolist(),\n",
    "    'model_name': model_name,\n",
    "    'run_id': run_id\n",
    "})\n",
    "results.to_csv(output_dir / \"prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.101673Z",
     "iopub.status.idle": "2025-07-15T10:07:10.101937Z",
     "shell.execute_reply": "2025-07-15T10:07:10.101792Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.101783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Save Metrics =====\n",
    "# Save raw evaluation metrics to JSON\n",
    "with open(output_dir / \"eval_metrics.json\", \"w\") as f:\n",
    "    json.dump(preds.metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.103127Z",
     "iopub.status.idle": "2025-07-15T10:07:10.103439Z",
     "shell.execute_reply": "2025-07-15T10:07:10.103293Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.103280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Save Report & Confusion Matrix =====\n",
    "# Save detailed classification report and confusion matrix\n",
    "conf_matrix = confusion_matrix(results['true_label'], results['pred_label'])\n",
    "report = classification_report(results['true_label'], results['pred_label'], output_dict=True)\n",
    "\n",
    "with open(output_dir / \"classification_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "with open(output_dir / \"confusion_matrix.json\", \"w\") as f:\n",
    "    json.dump(conf_matrix.tolist(), f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.105097Z",
     "iopub.status.idle": "2025-07-15T10:07:10.105337Z",
     "shell.execute_reply": "2025-07-15T10:07:10.105236Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.105224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Estimate learning rate & early stop epoch ===\n",
    "best_lr = trainer.optimizer.param_groups[0]['lr']\n",
    "steps_per_epoch = len(train_ds) // training_args.per_device_train_batch_size\n",
    "epoch_stopped_at = trainer.state.global_step // steps_per_epoch\n",
    "\n",
    "print(f\"Final learning rate (from best checkpoint): {best_lr}\")\n",
    "print(f\"Early stopped at epoch: {epoch_stopped_at}\")\n",
    "\n",
    "# save best_lr to file\n",
    "with open(output_dir / \"best_lr.txt\", \"w\") as f:\n",
    "    f.write(str(best_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.106563Z",
     "iopub.status.idle": "2025-07-15T10:07:10.106800Z",
     "shell.execute_reply": "2025-07-15T10:07:10.106691Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.106679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save model & training configuration before training\n",
    "config_snapshot = {\n",
    "    \"model_name\": model_name,\n",
    "    \"run_id\": run_id,\n",
    "\n",
    "     # Hyperparameters\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n",
    "    \"warmup_ratio\": training_args.warmup_ratio,\n",
    "    \"batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"num_epochs\": training_args.num_train_epochs,\n",
    "    \"weight_decay\": training_args.weight_decay,\n",
    "    \"early_stopping\": True,\n",
    "\n",
    "    \"best_lr\": best_lr,\n",
    "    \"epoch_stopped_at\": epoch_stopped_at,\n",
    "    \n",
    "    \"class_weights\": class_weights.cpu().tolist(),\n",
    "     \"datetime\": timestamp\n",
    "    #\"datetime\": datetime.now().isoformat()\n",
    "}\n",
    "with open(output_dir / \"config.json\", \"w\") as f:\n",
    "    json.dump(config_snapshot, f, indent=2)\n",
    "\n",
    "# Write human-readable run info for easy reference\n",
    "with open(output_dir / \"README.txt\", \"w\") as f:\n",
    "    f.write(f\"Run ID: {run_id}\\nModel: {model_name}\\nCreated: {timestamp}\\n\")\n",
    "    f.write(f\"Best LR (from checkpoint): {best_lr}\\nStopped at epoch: {epoch_stopped_at}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.108000Z",
     "iopub.status.idle": "2025-07-15T10:07:10.108252Z",
     "shell.execute_reply": "2025-07-15T10:07:10.108152Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.108140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Save TrainingArguments Params (for reproducibility) ===== #\n",
    "with open(output_dir / \"training_args.json\", \"w\") as f:\n",
    "    json.dump(training_args.to_dict(), f, indent=2)\n",
    "\n",
    "# ===== Print summary ===== #\n",
    "print(f\"All results saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "# ===== Append Main Metrics to Excel Summary Table ===== #\n",
    "# Create/update a summary Excel for final model comparisons\n",
    "summary_path = project_root / \"outputs\" / \"results\" / \"final_model_comparison.xlsx\"\n",
    "\n",
    "# Re-run prediction to ensure use final model output\n",
    "preds = trainer.predict(test_ds)\n",
    "y_true = preds.label_ids\n",
    "y_pred = preds.predictions.argmax(-1)\n",
    "\n",
    "# Compute detailed classification report\n",
    "report = classification_report(\n",
    "    y_true, y_pred,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "# Manually compute micro metrics to avoid missing values\n",
    "precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "# One row summarizing performance of this model run\n",
    "summary_row = {\n",
    "    \"model_name\": model_name,\n",
    "    \"run_id\": run_id,\n",
    "    \"accuracy\": preds.metrics.get(\"test_accuracy\", preds.metrics.get(\"eval_accuracy\", report[\"accuracy\"])),\n",
    " \n",
    "    # Per-class metrics\n",
    "    \"precision_0\": report['0']['precision'],\n",
    "    \"recall_0\": report['0']['recall'],\n",
    "    \"f1_0\": report['0']['f1-score'],\n",
    "    \"precision_1\": report['1']['precision'],\n",
    "    \"recall_1\": report['1']['recall'],\n",
    "    \"f1_1\": report['1']['f1-score'],\n",
    "\n",
    "    # Micro-average\n",
    "    \"precision_micro\": precision_micro,\n",
    "    \"recall_micro\": recall_micro,\n",
    "    \"f1_micro\": f1_micro,\n",
    "\n",
    "    # Macro-average\n",
    "    \"precision_macro\": report['macro avg']['precision'],\n",
    "    \"recall_macro\": report['macro avg']['recall'],\n",
    "    \"f1_macro\": report['macro avg']['f1-score'],\n",
    "\n",
    "    # Weighted Averages\n",
    "    \"precision_weighted\": report['weighted avg']['precision'], \n",
    "    \"recall_weighted\": report['weighted avg']['recall'], \n",
    "    \"f1_weighted\": report['weighted avg']['f1-score'],\n",
    "\n",
    "    \"lr_max\": training_args.learning_rate,\n",
    "    \"lr_min\": 0.0,  \n",
    "    \"best_lr\": best_lr,\n",
    "    \"epoch_stopped_at\": epoch_stopped_at,\n",
    "\n",
    "     # Class balance info\n",
    "    \"support_1\": report['1']['support'],\n",
    "    \"support_0\": report['0']['support'],\n",
    "    \"train_time_sec\": train_time\n",
    "\n",
    "}\n",
    "\n",
    "# ===== Excel Integration ===== #\n",
    "# Append or create Excel summary table for all runs\n",
    "\n",
    "df_row = pd.DataFrame([summary_row])\n",
    "if not summary_path.exists():\n",
    "    df_row.to_excel(summary_path, index=False)\n",
    "else:\n",
    "    existing = pd.read_excel(summary_path)\n",
    "    updated = pd.concat([existing, df_row], ignore_index=True)\n",
    "    updated.to_excel(summary_path, index=False)\n",
    "\n",
    "print(f\"Summary row saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.109357Z",
     "iopub.status.idle": "2025-07-15T10:07:10.109592Z",
     "shell.execute_reply": "2025-07-15T10:07:10.109499Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.109490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = \"/kaggle/working/MSc_Claim_Experiment/outputs/results\"\n",
    "\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.110125Z",
     "iopub.status.idle": "2025-07-15T10:07:10.110404Z",
     "shell.execute_reply": "2025-07-15T10:07:10.110251Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.110238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === 1. Define Base Path Depending on Platform ===\n",
    "if Path(\"/kaggle/working\").exists():\n",
    "    base_path = Path(\"/kaggle/working/MSc_Claim_Experiment/outputs/results\")\n",
    "elif Path(\"/content/drive\").exists():\n",
    "    base_path = Path(\"/content/drive/MyDrive/MSc_Claim_Experiment/outputs/results\")\n",
    "else:\n",
    "    base_path = Path(\"./outputs/results\")  # fallback for local dev\n",
    "\n",
    "# === 2. Load model summary file ===\n",
    "df_summary = pd.read_excel(base_path / \"final_model_comparison.xlsx\")\n",
    "print(\"Final Model Comparison:\")\n",
    "display(df_summary.head())\n",
    "\n",
    "# === 3. Define model_keyword (before calling the function) ===\n",
    "model_keyword = \"None\"  # Change to \"bert\", \"distilbert\", or None\n",
    "\n",
    "# === 4. Define reusable function ===\n",
    "def get_latest_run_folder(results_path: Path, model_keyword: str = None) -> Path:\n",
    "    \"\"\"\n",
    "    Get the most recently updated results folder.\n",
    "    If model_keyword is provided, only consider folders containing that keyword.\n",
    "    \"\"\"\n",
    "    subdirs = [d for d in results_path.iterdir() if d.is_dir()]\n",
    "    if model_keyword:\n",
    "        subdirs = [d for d in subdirs if model_keyword.lower() in d.name.lower()]\n",
    "    return max(subdirs, key=lambda d: d.stat().st_mtime) if subdirs else None\n",
    "\n",
    "# === 5. Get latest result folder and load predictions ===\n",
    "latest_run_folder = get_latest_run_folder(base_path, model_keyword=model_keyword)\n",
    "\n",
    "if latest_run_folder:\n",
    "    df_pred = pd.read_csv(latest_run_folder / \"prediction.csv\")\n",
    "    print(f\"Sample Predictions from: {latest_run_folder.name}\")\n",
    "    display(df_pred.head())\n",
    "else:\n",
    "    print(\"No valid result folder found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-07-15T10:07:10.111464Z",
     "iopub.status.idle": "2025-07-15T10:07:10.111765Z",
     "shell.execute_reply": "2025-07-15T10:07:10.111618Z",
     "shell.execute_reply.started": "2025-07-15T10:07:10.111600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# Backup all output files (including intermediate files, models, logs, etc.)\n",
    "# This creates a full archive of the experiment outputs\n",
    "shutil.make_archive(\n",
    "    \"/kaggle/working/results_backup\", \n",
    "    'zip', \n",
    "    \"/kaggle/working/MSc_Claim_Experiment/outputs\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7561111,
     "sourceId": 12018093,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7822271,
     "sourceId": 12403821,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7847747,
     "sourceId": 12440814,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
