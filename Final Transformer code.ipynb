{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:34.273295Z",
     "iopub.status.busy": "2025-04-19T18:49:34.272305Z",
     "iopub.status.idle": "2025-04-19T18:49:34.664563Z",
     "shell.execute_reply": "2025-04-19T18:49:34.663620Z",
     "shell.execute_reply.started": "2025-04-19T18:49:34.273259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/claims/claims.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:34.666462Z",
     "iopub.status.busy": "2025-04-19T18:49:34.666100Z",
     "iopub.status.idle": "2025-04-19T18:49:38.043985Z",
     "shell.execute_reply": "2025-04-19T18:49:38.042888Z",
     "shell.execute_reply.started": "2025-04-19T18:49:34.666441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT.ipynb\\n\\nAutomatically generated by Colab.\\n\\nOriginal file is located at\\n    https://colab.research.google.com/drive/1Q0YHA6RUjwOClr1AKzK3nUcPlVusNKGa\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required packages for Excel output\n",
    "!pip install -q openpyxl\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"BERT.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1Q0YHA6RUjwOClr1AKzK3nUcPlVusNKGa\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:38.045229Z",
     "iopub.status.busy": "2025-04-19T18:49:38.045027Z",
     "iopub.status.idle": "2025-04-19T18:49:39.932343Z",
     "shell.execute_reply": "2025-04-19T18:49:39.931544Z",
     "shell.execute_reply.started": "2025-04-19T18:49:38.045208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is Available\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is NOT available.\")\n",
    "    print(\"Please go to Runtime > Change runtime type > Select GPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:39.934654Z",
     "iopub.status.busy": "2025-04-19T18:49:39.934334Z",
     "iopub.status.idle": "2025-04-19T18:49:50.562056Z",
     "shell.execute_reply": "2025-04-19T18:49:50.561153Z",
     "shell.execute_reply.started": "2025-04-19T18:49:39.934635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745088587.069512    3703 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745088587.077101    3703 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets accelerate evaluate optuna\n",
    "\n",
    "# ===== Imports ===== #\n",
    "# Import all the libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import openpyxl\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from evaluate import load as load_metric\n",
    "from time import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:50.563568Z",
     "iopub.status.busy": "2025-04-19T18:49:50.563070Z",
     "iopub.status.idle": "2025-04-19T18:49:50.570850Z",
     "shell.execute_reply": "2025-04-19T18:49:50.570058Z",
     "shell.execute_reply.started": "2025-04-19T18:49:50.563542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Seed Everything (for full reproducibility) ===== #\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False  # This makes sure cuDNN doesnâ€™t auto-tune\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:50.572383Z",
     "iopub.status.busy": "2025-04-19T18:49:50.571770Z",
     "iopub.status.idle": "2025-04-19T18:49:50.585238Z",
     "shell.execute_reply": "2025-04-19T18:49:50.584396Z",
     "shell.execute_reply.started": "2025-04-19T18:49:50.572353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===== Configurations ===== #\n",
    "\n",
    "# model_name = \"bert-base-uncased\"  # Use the standard model\n",
    "# model_name = \"distilbert-base-uncased\"  \n",
    "# model_name = \"roberta-base\"  \n",
    "# model_name = \"albert-base-v2\" \n",
    "# model_name = \"xlnet-base-cased\" \n",
    "model_name = \"google/electra-base-discriminator\" \n",
    "\n",
    "# lr = 2e-5  # Learning rate (changeable)\n",
    "\n",
    "\n",
    "timestamp = datetime.now().isoformat()\n",
    "\n",
    "# Generate a unique run ID with model name and timestamp\n",
    "# run_id = f\"{model_name.replace('/', '_')}_lr{str(lr).replace('.', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "\n",
    "run_id = f\"{model_name.replace('/', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# ===== Environment-Aware Path Setup ===== #\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Running in Google Colab\n",
    "    project_root = Path(\"/content/drive/MyDrive/MSc_Claim_Experiment\")\n",
    "elif Path(\"/kaggle/working\").exists():\n",
    "    # Running in Kaggle\n",
    "    project_root = Path(\"/kaggle/working/MSc_Claim_Experiment\")\n",
    "else:\n",
    "    # Fallback for local development\n",
    "    project_root = Path(\"./MSc_Claim_Experiment\")\n",
    "\n",
    "# Define the output directory for the current run\n",
    "# output_dir = project_root / \"outputs\" / \"results\" / run_id\n",
    "output_dir = project_root / \"outputs\" / \"results\" / model_name.replace(\"/\", \"_\") / run_id\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create directory if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:50.586208Z",
     "iopub.status.busy": "2025-04-19T18:49:50.585988Z",
     "iopub.status.idle": "2025-04-19T18:49:50.697063Z",
     "shell.execute_reply": "2025-04-19T18:49:50.696475Z",
     "shell.execute_reply.started": "2025-04-19T18:49:50.586190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Load and Preprocess Data ===== #\n",
    "df = pd.read_csv(\"/kaggle/input/claims/claims.csv\", encoding=\"ISO-8859-1\", engine=\"python\")\n",
    "df['rating'] = df['rating'].str.lower().str.strip()  # Make ratings lowercase and remove spaces\n",
    "mapping = {\n",
    "    'true': 1,\n",
    "    'mostly-true': 1,\n",
    "    'false': 0,\n",
    "    'mostly-false': 0\n",
    "}  # Convert ratings to 0 or 1\n",
    "\n",
    "\n",
    "# Apply label mapping\n",
    "\n",
    "df['label_binary'] = df['rating'].map(mapping)\n",
    "df = df[df['label_binary'].isin([0, 1])] # Keep only rows that are 0 or 1 (drop unknowns)\n",
    "\n",
    "# Split into train, validation, test\n",
    "train_val, test = train_test_split(df, test_size=0.15, stratify=df['label_binary'], random_state=42)\n",
    "train, val      = train_test_split(train_val, test_size=0.1765, stratify=train_val['label_binary'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:50.698126Z",
     "iopub.status.busy": "2025-04-19T18:49:50.697811Z",
     "iopub.status.idle": "2025-04-19T18:49:50.881679Z",
     "shell.execute_reply": "2025-04-19T18:49:50.880916Z",
     "shell.execute_reply.started": "2025-04-19T18:49:50.698098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: label_binary\n",
      "0.0    0.759352\n",
      "1.0    0.240648\n",
      "Name: proportion, dtype: float64\n",
      "Val: label_binary\n",
      "0.0    0.761628\n",
      "1.0    0.238372\n",
      "Name: proportion, dtype: float64\n",
      "Test: label_binary\n",
      "0.0    0.761628\n",
      "1.0    0.238372\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ===== Compute Class Weights =====\n",
    "# Handle class imbalance by adjusting loss contribution of each class. Give rare class more weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train['label_binary']),\n",
    "    y=train['label_binary']\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Save class weights to JSON file for reproducibility\n",
    "with open(output_dir / \"class_weights.json\", \"w\") as f:\n",
    "    json.dump(class_weights.cpu().numpy().tolist(), f, indent=2)\n",
    "\n",
    "# Show class distribution\n",
    "print(\"Train:\", train['label_binary'].value_counts(normalize=True))\n",
    "print(\"Val:\", val['label_binary'].value_counts(normalize=True))\n",
    "print(\"Test:\", test['label_binary'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:50.882718Z",
     "iopub.status.busy": "2025-04-19T18:49:50.882497Z",
     "iopub.status.idle": "2025-04-19T18:49:51.886648Z",
     "shell.execute_reply": "2025-04-19T18:49:51.886126Z",
     "shell.execute_reply.started": "2025-04-19T18:49:50.882700Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56fbd086fb3411f99ba5995ede2fb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf040c14df34ce4b1a2453e97718a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0586b0e543640b1b3c6fe2ba17faad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b9c5aadf104288bb649469d7c18ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a649f9cd74e46d594dfc7a37ccedf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1146 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ===== Tokenizer ===== #\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load BERT tokenizer\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['statement'], padding=True, truncation=True)  # Turn text into tokens\n",
    "\n",
    "# Combine all data for consistent indexing\n",
    "dataset = Dataset.from_pandas(pd.concat([train, val, test]).reset_index(drop=True))\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "\n",
    "# Split tokenized data back into train, val, test\n",
    "train_dataset = tokenized_dataset.select(range(len(train)))\n",
    "val_dataset = tokenized_dataset.select(range(len(train), len(train)+len(val)))\n",
    "test_dataset = tokenized_dataset.select(range(len(train)+len(val), len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:51.889927Z",
     "iopub.status.busy": "2025-04-19T18:49:51.889506Z",
     "iopub.status.idle": "2025-04-19T18:49:51.895026Z",
     "shell.execute_reply": "2025-04-19T18:49:51.894267Z",
     "shell.execute_reply.started": "2025-04-19T18:49:51.889879Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===== Ensure Labels Are Integers (Fix for loss mismatch) ===== #\n",
    "train[\"label_binary\"] = train[\"label_binary\"].astype(int)\n",
    "val[\"label_binary\"] = val[\"label_binary\"].astype(int)\n",
    "test[\"label_binary\"] = test[\"label_binary\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:51.895851Z",
     "iopub.status.busy": "2025-04-19T18:49:51.895563Z",
     "iopub.status.idle": "2025-04-19T18:49:51.908273Z",
     "shell.execute_reply": "2025-04-19T18:49:51.907532Z",
     "shell.execute_reply.started": "2025-04-19T18:49:51.895826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Columns to Remove During Cleanup ===== #\n",
    "columns_to_remove = [\n",
    "    'statement', 'label_binary', 'id', 'name', 'description', 'category',\n",
    "    'rating', 'queries', 'created_at', 'updated_at',\n",
    "    'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:51.909250Z",
     "iopub.status.busy": "2025-04-19T18:49:51.909022Z",
     "iopub.status.idle": "2025-04-19T18:49:51.920798Z",
     "shell.execute_reply": "2025-04-19T18:49:51.920298Z",
     "shell.execute_reply.started": "2025-04-19T18:49:51.909226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary columns dynamically\n",
    "def safe_remove_columns(dataset, columns):\n",
    "    existing = [col for col in columns if col in dataset.column_names]\n",
    "    return dataset.remove_columns(existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:51.921746Z",
     "iopub.status.busy": "2025-04-19T18:49:51.921493Z",
     "iopub.status.idle": "2025-04-19T18:49:51.933462Z",
     "shell.execute_reply": "2025-04-19T18:49:51.932781Z",
     "shell.execute_reply.started": "2025-04-19T18:49:51.921724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Unified Tokenize + Clean Function ===== #\n",
    "def tokenize_and_clean(df):\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False) # Convert to HuggingFace Dataset format\n",
    "    ds = ds.rename_column(\"label_binary\", \"labels\") # Rename label column for compatibility\n",
    "    ds = ds.map(tokenize, batched=True)  # Apply tokenizer to every row\n",
    "    existing = [col for col in columns_to_remove if col in ds.column_names] # Check which columns can be removed\n",
    "    return ds.remove_columns(existing) # Return cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:51.934434Z",
     "iopub.status.busy": "2025-04-19T18:49:51.934191Z",
     "iopub.status.idle": "2025-04-19T18:49:52.214881Z",
     "shell.execute_reply": "2025-04-19T18:49:52.214081Z",
     "shell.execute_reply.started": "2025-04-19T18:49:51.934410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e225b868c99d48baa25e26fd168e29e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce24497aef18490ba939b95356a10321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca57ba0c72ec423aac7c1bcc4aa6e5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset columns: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# ===== Apply Processing Pipeline to Each Split ===== #\n",
    "train_ds = tokenize_and_clean(train)\n",
    "val_ds   = tokenize_and_clean(val)\n",
    "test_ds  = tokenize_and_clean(test)\n",
    "\n",
    "print(\"Final dataset columns:\", train_ds.column_names) # Only 'input_ids', 'attention_mask', 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:52.216112Z",
     "iopub.status.busy": "2025-04-19T18:49:52.215757Z",
     "iopub.status.idle": "2025-04-19T18:49:55.587243Z",
     "shell.execute_reply": "2025-04-19T18:49:55.586370Z",
     "shell.execute_reply.started": "2025-04-19T18:49:52.216092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31b0ac529aa44b6b75a16a4bc37f444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040e71ff37db416bb32bc7dbf07d0739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ===== Initialize Model ===== #\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:55.588881Z",
     "iopub.status.busy": "2025-04-19T18:49:55.588597Z",
     "iopub.status.idle": "2025-04-19T18:49:55.596973Z",
     "shell.execute_reply": "2025-04-19T18:49:55.595975Z",
     "shell.execute_reply.started": "2025-04-19T18:49:55.588860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "\n",
    "    y_true = pred.label_ids # The true labels of the test set\n",
    "    y_pred = pred.predictions.argmax(-1) # The model's predicted class (the one with the highest probability)\n",
    "\n",
    "    # Calculate evaluation metrics for class 0\n",
    "    precision = precision_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "\n",
    "    # Overall accuracy of the model\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate weighted average F1 score (accounts for class imbalance)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Calculate micro-averaged precision, recall, and F1 (global average across all classes)\n",
    "    precision_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    recall_micro = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    # Return all metrics in a dictionary\n",
    "    return {\n",
    "        'eval_f1': f1_weighted,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        # \"f1\": f1,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"precision_weighted\": precision_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"recall_weighted\": recall_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"support_1\": int((y_true == 1).sum()),  # Number of class 1 samples\n",
    "        \"support_0\": int((y_true == 0).sum())  # Number of class 0 samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:55.598688Z",
     "iopub.status.busy": "2025-04-19T18:49:55.598358Z",
     "iopub.status.idle": "2025-04-19T18:49:55.652356Z",
     "shell.execute_reply": "2025-04-19T18:49:55.651451Z",
     "shell.execute_reply.started": "2025-04-19T18:49:55.598657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Define Training Arguments ===== #\n",
    "# Key parameters for reproducibility and fine-tuning control\n",
    "# This block defines how training should proceed, how models are saved, and evaluation behavior\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,  # Folder to store model checkpoints and logs\n",
    "    # evaluation_strategy='epoch',  # Evaluate model after each epoch\n",
    "    eval_strategy='epoch', \n",
    "    save_strategy='epoch', # Save model after each epoch\n",
    "    save_total_limit=1, # Keep only the latest saved model to reduce disk usage\n",
    "    # learning_rate=2e-5,  # Initial learning rate for optimizer\n",
    "    # learning_rate=3e-5, # Alternative setting for grid search\n",
    "    learning_rate=5e-5, # # Max learning rate\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1, \n",
    "    per_device_train_batch_size=8,  # Number of samples per GPU/CPU during training\n",
    "    per_device_eval_batch_size=8,  # Number of samples per GPU/CPU during validation\n",
    "    num_train_epochs=100, # Max number of training epochs (early stopping can interrupt before 100)\n",
    "    weight_decay=0.01, # L2 regularization to reduce overfitting\n",
    "    load_best_model_at_end=True,  # Restore best model at the end of training based on eval metric\n",
    "    metric_for_best_model = 'eval_f1',\n",
    "    greater_is_better = True,\n",
    "    logging_dir=f\"logs/{run_id}\", # Directory to save training logs\n",
    "    logging_strategy='epoch', # Log metrics at the end of each epoch\n",
    "    seed=42, # Set random seed for reproducibility\n",
    "    fp16=True,  # Use mixed precision for faster training on supported GPUs\n",
    "    remove_unused_columns=False,  # Retain all columns to prevent loss of important features\n",
    "    report_to='none', # Do not report metrics to third-party tools (e.g., WandB)\n",
    "    disable_tqdm=True,  # Disable tqdm progress bar to reduce console clutter in Colab\n",
    "    eval_accumulation_steps=32,  # Accumulate gradients during evaluation to save memory\n",
    "    no_cuda=False  # Use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:55.653574Z",
     "iopub.status.busy": "2025-04-19T18:49:55.653304Z",
     "iopub.status.idle": "2025-04-19T18:49:55.660970Z",
     "shell.execute_reply": "2025-04-19T18:49:55.659828Z",
     "shell.execute_reply.started": "2025-04-19T18:49:55.653545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Custom Logging Callback for Per-Epoch Metrics ===== #\n",
    "# This callback logs evaluation metrics (loss, accuracy, precision, recall, F1) after every epoch\n",
    "# It writes the results into a CSV file \"epoch_log.csv\" for visualization or debugging\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "import csv\n",
    "\n",
    "class MetricsLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, log_file=output_dir / \"epoch_log.csv\"):\n",
    "        self.log_file = log_file\n",
    "        with open(self.log_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                \"epoch\", \"learning_rate\",\n",
    "                \"eval_loss\", \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
    "            ])\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None or \"eval_loss\" not in logs:\n",
    "            return  # Skip non-eval logs\n",
    "\n",
    "        current_lr = logs.get(\"learning_rate\", None)\n",
    "\n",
    "        with open(self.log_file, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                round(state.epoch, 2),\n",
    "                current_lr,\n",
    "                logs.get(\"eval_loss\"),\n",
    "                logs.get(\"eval_accuracy\"),\n",
    "                logs.get(\"eval_precision\"),\n",
    "                logs.get(\"eval_recall\"),\n",
    "                logs.get(\"eval_f1\")\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T19:58:37.998718Z",
     "iopub.status.busy": "2025-04-19T19:58:37.998252Z",
     "iopub.status.idle": "2025-04-19T19:59:10.538985Z",
     "shell.execute_reply": "2025-04-19T19:59:10.538090Z",
     "shell.execute_reply.started": "2025-04-19T19:58:37.998691Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 19:58:53.182645: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745092733.484710      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745092733.566766      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/945396725.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Instantiate Trainer with early stopping and epoch logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m trainer = CustomTrainer(\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== Custom Trainer with Class Weights =====\n",
    "#  Enables weighted loss to address label imbalance in binary classification\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Use weighted cross entropy loss\n",
    "        #loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        #loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
    "        \n",
    "        num_labels = model.module.config.num_labels if hasattr(model, \"module\") else model.config.num_labels\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "\n",
    "        # loss = loss_fct(logits.view(-1, model.module.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Instantiate Trainer with early stopping and epoch logger\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5), MetricsLoggerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:49:55.893343Z",
     "iopub.status.busy": "2025-04-19T18:49:55.893043Z",
     "iopub.status.idle": "2025-04-19T18:54:53.235685Z",
     "shell.execute_reply": "2025-04-19T18:54:53.234824Z",
     "shell.execute_reply.started": "2025-04-19T18:49:55.893318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6776, 'grad_norm': 248065.46875, 'learning_rate': 4.901960784313726e-06, 'epoch': 1.0}\n",
      "{'eval_f1': 0.550808975727147, 'eval_loss': 0.6644456386566162, 'eval_precision': 0.9166666666666666, 'eval_recall': 0.4198473282442748, 'eval_f1_weighted': 0.550808975727147, 'eval_accuracy': 0.5290697674418605, 'eval_precision_micro': 0.5290697674418605, 'eval_recall_micro': 0.5290697674418605, 'eval_precision_weighted': 0.7747785160575859, 'eval_recall_weighted': 0.5290697674418605, 'eval_f1_micro': 0.5290697674418605, 'eval_support_1': 41, 'eval_support_0': 131, 'eval_runtime': 2.2146, 'eval_samples_per_second': 77.666, 'eval_steps_per_second': 4.967, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6131, 'grad_norm': 462267.03125, 'learning_rate': 9.901960784313725e-06, 'epoch': 2.0}\n",
      "{'eval_f1': 0.6483583282848889, 'eval_loss': 0.5880460143089294, 'eval_precision': 0.9230769230769231, 'eval_recall': 0.549618320610687, 'eval_f1_weighted': 0.6483583282848889, 'eval_accuracy': 0.622093023255814, 'eval_precision_micro': 0.622093023255814, 'eval_recall_micro': 0.622093023255814, 'eval_precision_weighted': 0.7917967114528223, 'eval_recall_weighted': 0.622093023255814, 'eval_f1_micro': 0.622093023255814, 'eval_support_1': 41, 'eval_support_0': 131, 'eval_runtime': 2.1078, 'eval_samples_per_second': 81.603, 'eval_steps_per_second': 5.219, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.498, 'grad_norm': 673248.9375, 'learning_rate': 1.4901960784313726e-05, 'epoch': 3.0}\n",
      "{'eval_f1': 0.7481118287540012, 'eval_loss': 0.5620527863502502, 'eval_precision': 0.8583333333333333, 'eval_recall': 0.7862595419847328, 'eval_f1_weighted': 0.7481118287540012, 'eval_accuracy': 0.7383720930232558, 'eval_precision_micro': 0.7383720930232558, 'eval_recall_micro': 0.7383720930232558, 'eval_precision_weighted': 0.7637485092426953, 'eval_recall_weighted': 0.7383720930232558, 'eval_f1_micro': 0.7383720930232558, 'eval_support_1': 41, 'eval_support_0': 131, 'eval_runtime': 2.122, 'eval_samples_per_second': 81.055, 'eval_steps_per_second': 5.184, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3352, 'grad_norm': 295406.875, 'learning_rate': 1.9901960784313726e-05, 'epoch': 4.0}\n",
      "{'eval_f1': 0.7247426610751049, 'eval_loss': 0.6725161075592041, 'eval_precision': 0.8584070796460177, 'eval_recall': 0.7404580152671756, 'eval_f1_weighted': 0.7247426610751049, 'eval_accuracy': 0.7093023255813954, 'eval_precision_micro': 0.7093023255813954, 'eval_recall_micro': 0.7093023255813954, 'eval_precision_weighted': 0.7547919115672123, 'eval_recall_weighted': 0.7093023255813954, 'eval_f1_micro': 0.7093023255813953, 'eval_support_1': 41, 'eval_support_0': 131, 'eval_runtime': 2.343, 'eval_samples_per_second': 73.41, 'eval_steps_per_second': 4.695, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1973, 'grad_norm': 605834.5625, 'learning_rate': 2.4901960784313726e-05, 'epoch': 5.0}\n",
      "{'eval_f1': 0.809436823685104, 'eval_loss': 0.9322278499603271, 'eval_precision': 0.852112676056338, 'eval_recall': 0.9236641221374046, 'eval_f1_weighted': 0.809436823685104, 'eval_accuracy': 0.8197674418604651, 'eval_precision_micro': 0.8197674418604651, 'eval_recall_micro': 0.8197674418604651, 'eval_precision_weighted': 0.8079075226553116, 'eval_recall_weighted': 0.8197674418604651, 'eval_f1_micro': 0.8197674418604651, 'eval_support_1': 41, 'eval_support_0': 131, 'eval_runtime': 2.2353, 'eval_samples_per_second': 76.946, 'eval_steps_per_second': 4.921, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1544, 'grad_norm': 9472.1279296875, 'learning_rate': 2.9901960784313725e-05, 'epoch': 6.0}\n",
      "{'eval_f1': 0.7937707641196013, 'eval_loss': 1.452718734741211, 'eval_precision': 0.8322147651006712, 'eval_recall': 0.9465648854961832, 'eval_f1_weighted': 0.7937707641196013, 'eval_accuracy': 0.813953488372093, 'eval_precision_micro': 0.813953488372093, 'eval_recall_micro': 0.813953488372093, 'eval_precision_weighted': 0.799662054410597, 'eval_recall_weighted': 0.813953488372093, 'eval_f1_micro': 0.8139534883720931, 'eval_support_1': 41, 'eval_support_0': 131, 'eval_runtime': 2.4106, 'eval_samples_per_second': 71.35, 'eval_steps_per_second': 4.563, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1024, 'grad_norm': 324.8459167480469, 'learning_rate': 3.4901960784313725e-05, 'epoch': 7.0}\n",
      "{'eval_f1': 0.7741873669497557, 'eval_loss': 2.0158889293670654, 'eval_precision': 0.8538461538461538, 'eval_recall': 0.8473282442748091, 'eval_f1_weighted': 0.7741873669497557, 'eval_accuracy': 0.7732558139534884, 'eval_precision_micro': 0.7732558139534884, 'eval_recall_micro': 0.7732558139534884, 'eval_precision_weighted': 0.7751746315699805, 'eval_recall_weighted': 0.7732558139534884, 'eval_f1_micro': 0.7732558139534884, 'eval_support_1': 41, 'eval_support_0': 131, 'eval_runtime': 2.2325, 'eval_samples_per_second': 77.043, 'eval_steps_per_second': 4.927, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.143, 'grad_norm': 3.67734694480896, 'learning_rate': 3.990196078431373e-05, 'epoch': 8.0}\n",
      "{'eval_f1': 0.7967354171614388, 'eval_loss': 3.566232681274414, 'eval_precision': 0.8540145985401459, 'eval_recall': 0.8931297709923665, 'eval_f1_weighted': 0.7967354171614388, 'eval_accuracy': 0.8023255813953488, 'eval_precision_micro': 0.8023255813953488, 'eval_recall_micro': 0.8023255813953488, 'eval_precision_weighted': 0.7934646070276693, 'eval_recall_weighted': 0.8023255813953488, 'eval_f1_micro': 0.8023255813953488, 'eval_support_1': 41, 'eval_support_0': 131, 'eval_runtime': 2.3478, 'eval_samples_per_second': 73.261, 'eval_steps_per_second': 4.685, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.165, 'grad_norm': 0.014997675083577633, 'learning_rate': 4.490196078431373e-05, 'epoch': 9.0}\n",
      "{'eval_f1': 0.7186175392370261, 'eval_loss': 4.709323406219482, 'eval_precision': 0.8910891089108911, 'eval_recall': 0.6870229007633588, 'eval_f1_weighted': 0.7186175392370261, 'eval_accuracy': 0.6976744186046512, 'eval_precision_micro': 0.6976744186046512, 'eval_recall_micro': 0.6976744186046512, 'eval_precision_weighted': 0.7793989356354568, 'eval_recall_weighted': 0.6976744186046512, 'eval_f1_micro': 0.6976744186046512, 'eval_support_1': 41, 'eval_support_0': 131, 'eval_runtime': 2.1455, 'eval_samples_per_second': 80.167, 'eval_steps_per_second': 5.127, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2859, 'grad_norm': 0.06002817302942276, 'learning_rate': 4.990196078431373e-05, 'epoch': 10.0}\n",
      "{'eval_f1': 0.7556185921480371, 'eval_loss': 4.158874988555908, 'eval_precision': 0.8717948717948718, 'eval_recall': 0.7786259541984732, 'eval_f1_weighted': 0.7556185921480371, 'eval_accuracy': 0.7441860465116279, 'eval_precision_micro': 0.7441860465116279, 'eval_recall_micro': 0.7441860465116279, 'eval_precision_weighted': 0.7766682929473627, 'eval_recall_weighted': 0.7441860465116279, 'eval_f1_micro': 0.7441860465116278, 'eval_support_1': 41, 'eval_support_0': 131, 'eval_runtime': 2.2151, 'eval_samples_per_second': 77.648, 'eval_steps_per_second': 4.966, 'epoch': 10.0}\n",
      "{'train_runtime': 296.633, 'train_samples_per_second': 270.368, 'train_steps_per_second': 17.193, 'train_loss': 0.31720276065901215, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# ===== Train Model with Timer ===== #\n",
    "# Record how long training took\n",
    "start_time = time()\n",
    "trainer.train()\n",
    "end_time = time()\n",
    "train_time = round(end_time - start_time, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:53.237097Z",
     "iopub.status.busy": "2025-04-19T18:54:53.236704Z",
     "iopub.status.idle": "2025-04-19T18:54:53.243824Z",
     "shell.execute_reply": "2025-04-19T18:54:53.243059Z",
     "shell.execute_reply.started": "2025-04-19T18:54:53.237068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Save training loss history (for reproducibility / visualization)\n",
    "with open(output_dir / \"train_loss_curve.json\", \"w\") as f:\n",
    "    json.dump(trainer.state.log_history, f, indent=2)\n",
    "\n",
    "# ===== Save training time ===== #\n",
    "with open(output_dir / \"training_time_log.json\", \"w\") as f:\n",
    "    json.dump({\"train_time_seconds\": train_time}, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:53.244710Z",
     "iopub.status.busy": "2025-04-19T18:54:53.244531Z",
     "iopub.status.idle": "2025-04-19T18:54:53.258382Z",
     "shell.execute_reply": "2025-04-19T18:54:53.257379Z",
     "shell.execute_reply.started": "2025-04-19T18:54:53.244696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: electra, current LR: 5e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model loaded from: {trainer.model.base_model_prefix}, current LR: {trainer.optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:53.259585Z",
     "iopub.status.busy": "2025-04-19T18:54:53.259309Z",
     "iopub.status.idle": "2025-04-19T18:54:55.441319Z",
     "shell.execute_reply": "2025-04-19T18:54:55.440769Z",
     "shell.execute_reply.started": "2025-04-19T18:54:53.259560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# ===== Predict and Save Outputs ===== #\n",
    "# Generate predictions from best model on the test set\n",
    "preds = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:55.442382Z",
     "iopub.status.busy": "2025-04-19T18:54:55.442121Z",
     "iopub.status.idle": "2025-04-19T18:54:56.109169Z",
     "shell.execute_reply": "2025-04-19T18:54:56.108520Z",
     "shell.execute_reply.started": "2025-04-19T18:54:55.442361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the best model weights for reproducibility or future use\n",
    "trainer.save_model(project_root / \"outputs\" / \"results\" / \"saved_models\" / f\"{model_name.replace('/', '_')}_{run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:56.110137Z",
     "iopub.status.busy": "2025-04-19T18:54:56.109870Z",
     "iopub.status.idle": "2025-04-19T18:54:56.114355Z",
     "shell.execute_reply": "2025-04-19T18:54:56.113762Z",
     "shell.execute_reply.started": "2025-04-19T18:54:56.110119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract predicted labels\n",
    "preds_labels = np.argmax(preds.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:56.115631Z",
     "iopub.status.busy": "2025-04-19T18:54:56.115271Z",
     "iopub.status.idle": "2025-04-19T18:54:56.133943Z",
     "shell.execute_reply": "2025-04-19T18:54:56.133280Z",
     "shell.execute_reply.started": "2025-04-19T18:54:56.115605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Save Raw Prediction Table ===== #\n",
    "# Save full test set results with input, true label, predicted label, and logits\n",
    "results = pd.DataFrame({\n",
    "    'statement': test['statement'].reset_index(drop=True),\n",
    "    'true_label': test['label_binary'].reset_index(drop=True),\n",
    "    'pred_label': preds_labels,\n",
    "    'logits': preds.predictions.tolist(),\n",
    "    'model_name': model_name,\n",
    "    'run_id': run_id\n",
    "})\n",
    "results.to_csv(output_dir / \"prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:56.135168Z",
     "iopub.status.busy": "2025-04-19T18:54:56.134847Z",
     "iopub.status.idle": "2025-04-19T18:54:56.139663Z",
     "shell.execute_reply": "2025-04-19T18:54:56.138948Z",
     "shell.execute_reply.started": "2025-04-19T18:54:56.135148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Save Metrics =====\n",
    "# Save raw evaluation metrics to JSON\n",
    "with open(output_dir / \"eval_metrics.json\", \"w\") as f:\n",
    "    json.dump(preds.metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:56.143740Z",
     "iopub.status.busy": "2025-04-19T18:54:56.143243Z",
     "iopub.status.idle": "2025-04-19T18:54:56.160871Z",
     "shell.execute_reply": "2025-04-19T18:54:56.160060Z",
     "shell.execute_reply.started": "2025-04-19T18:54:56.143722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Save Report & Confusion Matrix =====\n",
    "# Save detailed classification report and confusion matrix\n",
    "conf_matrix = confusion_matrix(results['true_label'], results['pred_label'])\n",
    "report = classification_report(results['true_label'], results['pred_label'], output_dict=True)\n",
    "\n",
    "with open(output_dir / \"classification_report.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "with open(output_dir / \"confusion_matrix.json\", \"w\") as f:\n",
    "    json.dump(conf_matrix.tolist(), f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:56.161738Z",
     "iopub.status.busy": "2025-04-19T18:54:56.161564Z",
     "iopub.status.idle": "2025-04-19T18:54:56.167683Z",
     "shell.execute_reply": "2025-04-19T18:54:56.166940Z",
     "shell.execute_reply.started": "2025-04-19T18:54:56.161723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final learning rate (from best checkpoint): 5e-05\n",
      "Early stopped at epoch: 5\n"
     ]
    }
   ],
   "source": [
    "# === Estimate learning rate & early stop epoch ===\n",
    "best_lr = trainer.optimizer.param_groups[0]['lr']\n",
    "steps_per_epoch = len(train_ds) // training_args.per_device_train_batch_size\n",
    "epoch_stopped_at = trainer.state.global_step // steps_per_epoch\n",
    "\n",
    "print(f\"Final learning rate (from best checkpoint): {best_lr}\")\n",
    "print(f\"Early stopped at epoch: {epoch_stopped_at}\")\n",
    "\n",
    "# save best_lr to file\n",
    "with open(output_dir / \"best_lr.txt\", \"w\") as f:\n",
    "    f.write(str(best_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:56.168840Z",
     "iopub.status.busy": "2025-04-19T18:54:56.168432Z",
     "iopub.status.idle": "2025-04-19T18:54:56.178947Z",
     "shell.execute_reply": "2025-04-19T18:54:56.178027Z",
     "shell.execute_reply.started": "2025-04-19T18:54:56.168816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save model & training configuration before training\n",
    "config_snapshot = {\n",
    "    \"model_name\": model_name,\n",
    "    \"run_id\": run_id,\n",
    "\n",
    "     # Hyperparameters\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n",
    "    \"warmup_ratio\": training_args.warmup_ratio,\n",
    "    \"batch_size\": training_args.per_device_train_batch_size,\n",
    "    \"num_epochs\": training_args.num_train_epochs,\n",
    "    \"weight_decay\": training_args.weight_decay,\n",
    "    \"early_stopping\": True,\n",
    "\n",
    "    \"best_lr\": best_lr,\n",
    "    \"epoch_stopped_at\": epoch_stopped_at,\n",
    "    \n",
    "    \"class_weights\": class_weights.cpu().tolist(),\n",
    "     \"datetime\": timestamp\n",
    "    #\"datetime\": datetime.now().isoformat()\n",
    "}\n",
    "with open(output_dir / \"config.json\", \"w\") as f:\n",
    "    json.dump(config_snapshot, f, indent=2)\n",
    "\n",
    "# Write human-readable run info for easy reference\n",
    "with open(output_dir / \"README.txt\", \"w\") as f:\n",
    "    f.write(f\"Run ID: {run_id}\\nModel: {model_name}\\nCreated: {timestamp}\\n\")\n",
    "    f.write(f\"Best LR (from checkpoint): {best_lr}\\nStopped at epoch: {epoch_stopped_at}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:56.180226Z",
     "iopub.status.busy": "2025-04-19T18:54:56.179896Z",
     "iopub.status.idle": "2025-04-19T18:54:58.230855Z",
     "shell.execute_reply": "2025-04-19T18:54:58.230098Z",
     "shell.execute_reply.started": "2025-04-19T18:54:56.180199Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results saved to: /kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary row saved to /kaggle/working/MSc_Claim_Experiment/outputs/results/final_model_comparison.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ===== Save TrainingArguments Params (for reproducibility) ===== #\n",
    "with open(output_dir / \"training_args.json\", \"w\") as f:\n",
    "    json.dump(training_args.to_dict(), f, indent=2)\n",
    "\n",
    "# ===== Print summary ===== #\n",
    "print(f\"All results saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "# ===== Append Main Metrics to Excel Summary Table ===== #\n",
    "# Create/update a summary Excel for final model comparisons\n",
    "summary_path = project_root / \"outputs\" / \"results\" / \"final_model_comparison.xlsx\"\n",
    "\n",
    "# Re-run prediction to ensure use final model output\n",
    "preds = trainer.predict(test_ds)\n",
    "y_true = preds.label_ids\n",
    "y_pred = preds.predictions.argmax(-1)\n",
    "\n",
    "# Compute detailed classification report\n",
    "report = classification_report(\n",
    "    y_true, y_pred,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "# Manually compute micro metrics to avoid missing values\n",
    "precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "# One row summarizing performance of this model run\n",
    "summary_row = {\n",
    "    \"model_name\": model_name,\n",
    "    \"run_id\": run_id,\n",
    "    \"accuracy\": preds.metrics.get(\"test_accuracy\", preds.metrics.get(\"eval_accuracy\", report[\"accuracy\"])),\n",
    "\n",
    "    # Per-class metrics\n",
    "    \"precision_0\": report['0']['precision'],\n",
    "    \"recall_0\": report['0']['recall'],\n",
    "    \"f1_0\": report['0']['f1-score'],\n",
    "    \"precision_1\": report['1']['precision'],\n",
    "    \"recall_1\": report['1']['recall'],\n",
    "    \"f1_1\": report['1']['f1-score'],\n",
    "\n",
    "    # Micro-average\n",
    "    \"precision_micro\": precision_micro,\n",
    "    \"recall_micro\": recall_micro,\n",
    "    \"f1_micro\": f1_micro,\n",
    "\n",
    "    # Averages\n",
    "    \"f1_macro\": report['macro avg']['f1-score'],\n",
    "    \"f1_weighted\": report['weighted avg']['f1-score'],\n",
    "    \"precision_weighted\": report['weighted avg']['precision'], \n",
    "    \"recall_weighted\": report['weighted avg']['recall'], \n",
    "\n",
    "    \"lr_max\": 5e-5,\n",
    "    \"lr_min\": 1e-6,  \n",
    "    \"best_lr\": best_lr,\n",
    "    \"epoch_stopped_at\": epoch_stopped_at,\n",
    "\n",
    "     # Class balance info\n",
    "    \"support_1\": report['1']['support'],\n",
    "    \"support_0\": report['0']['support'],\n",
    "    \"train_time_sec\": train_time\n",
    "\n",
    "}\n",
    "\n",
    "# ===== Excel Integration ===== #\n",
    "# Append or create Excel summary table for all runs\n",
    "\n",
    "df_row = pd.DataFrame([summary_row])\n",
    "if not summary_path.exists():\n",
    "    df_row.to_excel(summary_path, index=False)\n",
    "else:\n",
    "    existing = pd.read_excel(summary_path)\n",
    "    updated = pd.concat([existing, df_row], ignore_index=True)\n",
    "    updated.to_excel(summary_path, index=False)\n",
    "\n",
    "print(f\"Summary row saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:58.231841Z",
     "iopub.status.busy": "2025-04-19T18:54:58.231654Z",
     "iopub.status.idle": "2025-04-19T18:54:58.237824Z",
     "shell.execute_reply": "2025-04-19T18:54:58.237271Z",
     "shell.execute_reply.started": "2025-04-19T18:54:58.231826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/final_model_comparison.xlsx\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/README.txt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/confusion_matrix.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/class_weights.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/config.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/best_lr.txt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/classification_report.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/eval_metrics.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/training_args.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/epoch_log.csv\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/train_loss_curve.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/prediction.csv\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/training_time_log.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/checkpoint-255/model.safetensors\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/checkpoint-255/config.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/checkpoint-255/training_args.bin\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/checkpoint-255/trainer_state.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/checkpoint-255/optimizer.pt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/checkpoint-255/scheduler.pt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/checkpoint-255/rng_state.pth\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/google_electra-base-discriminator/google_electra-base-discriminator_20250419_184950/checkpoint-255/scaler.pt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/saved_models/google_electra-base-discriminator_google_electra-base-discriminator_20250419_184950/model.safetensors\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/saved_models/google_electra-base-discriminator_google_electra-base-discriminator_20250419_184950/config.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/saved_models/google_electra-base-discriminator_google_electra-base-discriminator_20250419_184950/training_args.bin\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/saved_models/albert-base-v2_albert-base-v2_20250419_182934/model.safetensors\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/saved_models/albert-base-v2_albert-base-v2_20250419_182934/config.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/saved_models/albert-base-v2_albert-base-v2_20250419_182934/training_args.bin\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/saved_models/xlnet-base-cased_xlnet-base-cased_20250419_183811/model.safetensors\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/saved_models/xlnet-base-cased_xlnet-base-cased_20250419_183811/config.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/saved_models/xlnet-base-cased_xlnet-base-cased_20250419_183811/training_args.bin\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/README.txt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/confusion_matrix.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/class_weights.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/config.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/best_lr.txt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/classification_report.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/eval_metrics.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/training_args.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/epoch_log.csv\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/train_loss_curve.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/prediction.csv\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/training_time_log.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/checkpoint-102/model.safetensors\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/checkpoint-102/config.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/checkpoint-102/training_args.bin\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/checkpoint-102/trainer_state.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/checkpoint-102/optimizer.pt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/checkpoint-102/scheduler.pt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/checkpoint-102/rng_state.pth\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/albert-base-v2/albert-base-v2_20250419_182934/checkpoint-102/scaler.pt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/README.txt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/confusion_matrix.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/class_weights.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/config.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/best_lr.txt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/classification_report.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/eval_metrics.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/training_args.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/epoch_log.csv\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/train_loss_curve.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/prediction.csv\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/training_time_log.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/checkpoint-255/model.safetensors\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/checkpoint-255/config.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/checkpoint-255/training_args.bin\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/checkpoint-255/trainer_state.json\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/checkpoint-255/optimizer.pt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/checkpoint-255/scheduler.pt\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/checkpoint-255/rng_state.pth\n",
      "/kaggle/working/MSc_Claim_Experiment/outputs/results/xlnet-base-cased/xlnet-base-cased_20250419_183811/checkpoint-255/scaler.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_path = \"/kaggle/working/MSc_Claim_Experiment/outputs/results\"\n",
    "\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:58.238532Z",
     "iopub.status.busy": "2025-04-19T18:54:58.238331Z",
     "iopub.status.idle": "2025-04-19T18:54:58.277139Z",
     "shell.execute_reply": "2025-04-19T18:54:58.276554Z",
     "shell.execute_reply.started": "2025-04-19T18:54:58.238516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>run_id</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>...</th>\n",
       "      <th>f1_weighted</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>lr_max</th>\n",
       "      <th>lr_min</th>\n",
       "      <th>best_lr</th>\n",
       "      <th>epoch_stopped_at</th>\n",
       "      <th>support_1</th>\n",
       "      <th>support_0</th>\n",
       "      <th>train_time_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>albert-base-v2_20250419_182934</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.843972</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792232</td>\n",
       "      <td>0.788891</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>131</td>\n",
       "      <td>93.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xlnet-base-cased</td>\n",
       "      <td>xlnet-base-cased_20250419_183811</td>\n",
       "      <td>0.854651</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.916031</td>\n",
       "      <td>0.90566</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.854651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852714</td>\n",
       "      <td>0.851424</td>\n",
       "      <td>0.854651</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>131</td>\n",
       "      <td>358.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google/electra-base-discriminator</td>\n",
       "      <td>google_electra-base-discriminator_20250419_184950</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0.87218</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798743</td>\n",
       "      <td>0.796171</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>131</td>\n",
       "      <td>297.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model_name  \\\n",
       "0                     albert-base-v2   \n",
       "1                   xlnet-base-cased   \n",
       "2  google/electra-base-discriminator   \n",
       "\n",
       "                                              run_id  accuracy  precision_0  \\\n",
       "0                     albert-base-v2_20250419_182934  0.802326     0.843972   \n",
       "1                   xlnet-base-cased_20250419_183811  0.854651     0.895522   \n",
       "2  google_electra-base-discriminator_20250419_184950  0.802326     0.859259   \n",
       "\n",
       "   recall_0     f1_0  precision_1  recall_1      f1_1  precision_micro  ...  \\\n",
       "0  0.908397  0.87500     0.612903  0.463415  0.527778         0.802326  ...   \n",
       "1  0.916031  0.90566     0.710526  0.658537  0.683544         0.854651  ...   \n",
       "2  0.885496  0.87218     0.594595  0.536585  0.564103         0.802326  ...   \n",
       "\n",
       "   f1_weighted  precision_weighted  recall_weighted   lr_max    lr_min  \\\n",
       "0     0.792232            0.788891         0.802326  0.00005  0.000001   \n",
       "1     0.852714            0.851424         0.854651  0.00005  0.000001   \n",
       "2     0.798743            0.796171         0.802326  0.00005  0.000001   \n",
       "\n",
       "    best_lr  epoch_stopped_at  support_1  support_0  train_time_sec  \n",
       "0  0.000035                 3         41        131           93.88  \n",
       "1  0.000050                 5         41        131          358.68  \n",
       "2  0.000050                 5         41        131          297.34  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid result folder found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === 1. Define Base Path Depending on Platform ===\n",
    "if Path(\"/kaggle/working\").exists():\n",
    "    base_path = Path(\"/kaggle/working/MSc_Claim_Experiment/outputs/results\")\n",
    "elif Path(\"/content/drive\").exists():\n",
    "    base_path = Path(\"/content/drive/MyDrive/MSc_Claim_Experiment/outputs/results\")\n",
    "else:\n",
    "    base_path = Path(\"./outputs/results\")  # fallback for local dev\n",
    "\n",
    "# === 2. Load model summary file ===\n",
    "df_summary = pd.read_excel(base_path / \"final_model_comparison.xlsx\")\n",
    "print(\"Final Model Comparison:\")\n",
    "display(df_summary.head())\n",
    "\n",
    "# === 3. Define model_keyword (before calling the function) ===\n",
    "model_keyword = \"None\"  # Change to \"bert\", \"distilbert\", or None\n",
    "\n",
    "# === 4. Define reusable function ===\n",
    "def get_latest_run_folder(results_path: Path, model_keyword: str = None) -> Path:\n",
    "    \"\"\"\n",
    "    Get the most recently updated results folder.\n",
    "    If model_keyword is provided, only consider folders containing that keyword.\n",
    "    \"\"\"\n",
    "    subdirs = [d for d in results_path.iterdir() if d.is_dir()]\n",
    "    if model_keyword:\n",
    "        subdirs = [d for d in subdirs if model_keyword.lower() in d.name.lower()]\n",
    "    return max(subdirs, key=lambda d: d.stat().st_mtime) if subdirs else None\n",
    "\n",
    "# === 5. Get latest result folder and load predictions ===\n",
    "latest_run_folder = get_latest_run_folder(base_path, model_keyword=model_keyword)\n",
    "\n",
    "if latest_run_folder:\n",
    "    df_pred = pd.read_csv(latest_run_folder / \"prediction.csv\")\n",
    "    print(f\"Sample Predictions from: {latest_run_folder.name}\")\n",
    "    display(df_pred.head())\n",
    "else:\n",
    "    print(\"No valid result folder found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:54:58.278117Z",
     "iopub.status.busy": "2025-04-19T18:54:58.277822Z",
     "iopub.status.idle": "2025-04-19T18:54:58.281556Z",
     "shell.execute_reply": "2025-04-19T18:54:58.280955Z",
     "shell.execute_reply.started": "2025-04-19T18:54:58.278089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# Backup all output files (including intermediate files, models, logs, etc.)\n",
    "# This creates a full archive of the experiment outputs\n",
    "# shutil.make_archive(\n",
    "#     \"/kaggle/working/results_backup\", \n",
    "#    'zip', \n",
    "#    \"/kaggle/working/MSc_Claim_Experiment/outputs\"\n",
    "# )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7156263,
     "sourceId": 11426237,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
